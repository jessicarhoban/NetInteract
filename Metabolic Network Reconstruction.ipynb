{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metabolic Network Reconstruction\n",
    "### Conservation of hierarchical structuring and multi-source compatibility\n",
    "\n",
    "Updated May 12<sup>th</sup>, 2021 <br>\n",
    "<i>Jessica Hoban, The Russell Lab at Drexel University</i>\n",
    "\n",
    "\n",
    "---\n",
    "## Contents: <br>\n",
    "\n",
    ">**[Purpose and Overview](#Overview)** <br><br>\n",
    ">**[Input](#Input)** <br><br>\n",
    ">**[Setup](#Setup)** <br>\n",
    "\n",
    "> **[PART A: Genome Annotation](#PartA)**<br><br>\n",
    ">> **[Step 1: Building the Directory](#Step1)**<br><br>\n",
    "> **[Step 2: If your genome is in KEGG](#Step2_KEGG)**<br><br>\n",
    "> **[Step 2: If your genome is in IMG](#Step2_IMG)**<br><br>\n",
    "> **[Step 2: If your genome is in NCBI](#Step2_NCBI)**<br><br>\n",
    "> **[Full Pipeline for Part A](#Full_Pipeline_PartA)**<br>\n",
    "\n",
    ">**[PART B: Network Reconstruction](#PartB)**<br><br>\n",
    ">> **[Step 3: Filtering and Extracting Pathway Data](#Step3)**<br><br>\n",
    "> **[Step 4: Determing Seeds/Nonseeds and Children/Parents](#Step4)**<br><br>\n",
    "> **[Step 5: Send to Files](#Step5)**<br><br>\n",
    "> **[Step 6: Visualize Network](#Step6)**<br><br>\n",
    "> **[Step 7: Summary Statistics and Network Comparisons](#Step7)**<br><br>\n",
    "> **[Full Pipeline for Part B](#Full_Pipeline_PartB)**<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Overview'></a>\n",
    "## <span style=\"margin:auto;display:table\">Purpose and Overview</span>\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook is an end-to-end pipeline for reconstructing metabolic networks, designed to be run in conjunction with the **NetInteract** tool. It is compatible with genomes that have annotations in either the Kyoto Encyclopedia of Genes and Genomes (**KEGG**) <sup>1</sup>, the Joint Genome Institute's Integrated Microbial Genomes system (**IMG**) <sup>2</sup>, or the National Center for Biotechnology Information (**NCBI**) <sup>3</sup>, as well as user-supplied genome annotations.\n",
    "    \n",
    "Standard representation for metabolic networks is edge-notation, with substrates and products separated by a tab. However, this formatting does not maintain the <i>origin</i> of those reactions. To understand which overall categories of metabolism (e.g. carbohydrates, amino acids, lipids, etc.) or specific pathways these reactions originated from, the data must be structured hierarchically. Not only can edge-notation files still be extracted in this structure—they can be filtered.\n",
    "\n",
    "The end data structure is a JSON file that maintains the hierarchical structure of category → pathway → reaction → compound, with additional distinctions between exogenous metabolites (\"seeds\") and endogenous metabolites (\"non-seeds\") as defined by the Borenstein lab<sup>4,5</sup>. For optimal user exploration, we also print out CSV files that can be easily browsed.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Overview of Process\n",
    "\n",
    "There are 5 required and several optional steps in this notebook.\n",
    "\n",
    "> **PART A: Genome Annotation** <br>\n",
    ">> **1)** Build a custom directory to hold data files. <br><br>\n",
    "> **2)** Retrieve all KEGG Orthology numbers (representing organismal genes) for each genome and send to file. There is a distinct implementation for each data source, though the outcome is the same. For more details, see each data source's corresponding section. <br><br>\n",
    "> <span style=\"margin:auto;display:table;color:blue;\"><i>User can then modify the genome annotation results from Part A.</i></span>\n",
    "\n",
    "> **PART B: Network Reconstruction** <br>\n",
    ">> **3)** For each pathway, connect to the KEGG API and retrieve the KGML file. Filter this file by the organism's KO numbers. Retrieve all matching reactions and their compounds. <br><br>\n",
    "> **4)** Determine \"seed\" (required) and \"non-seed\" (non-required) compounds using Kosaraju's algorithm. (<i>Theory borrowed from the Borenstein lab<sup>4,5</sup>. Code inspired by the tool PhyloMint <sup>6</sup>).</i> Additionally, determine predecessor/successor relationships between metabolites. <br><br>\n",
    "> **5)** Send to files (JSON and CSV).<br><br>\n",
    "> **6)** Visualize each metabolic network and send image to PDF file.<br><br>\n",
    "> **7)** Compute summary statistics for each network as well as pair-wise comparisons between all networks.\n",
    "\n",
    "---\n",
    "\n",
    "<b>References / Additional Resources:</b><br>\n",
    "><sup>1</sup> https://www.kegg.jp/ <br>\n",
    "<sup>2</sup> https://img.jgi.doe.gov/cgi-bin/mer/main.cgi <br>\n",
    "<sup>3</sup> https://www.ncbi.nlm.nih.gov/ <br>\n",
    "<sup>4</sup> https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0588-y <br>\n",
    "<sup>5</sup> http://depts.washington.edu/elbogs/NetCooperate/NetCooperateWeb.cgi<br>\n",
    "<sup>6</sup> https://github.com/mgtools/PhyloMint\n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Input'></a>\n",
    "## <span style=\"margin:auto;display:table\">Input</span>\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "This section will walk you through the required input for this notebook, which is broken down into 4 main steps:\n",
    ">**[1) Genome Information](#1GI)** <br><br>\n",
    ">**[2) For any genomes in the IMG database](#2IMG)** <br><br>\n",
    ">**[3) For any genomes in the NCBI database](#3NCBI)** <br><br>\n",
    ">**[4) For any user-supplied genomes](#4User)** <br>\n",
    "\n",
    "\n",
    "<img src=\"./Images/Input - Input Folder.png\" style=\"width: 50%;\">\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "<a id='1GI'></a>\n",
    "### 1) Genome Information\n",
    "\n",
    ">In the **\"Input\"** folder, you will find a CSV file called **\"Genome Information.csv\"**. It contains the following columns:\n",
    "- ```Organism Name``` <br>\n",
    "- ```Organism Type```<br>\n",
    "- ```NCBI ID```  <br>\n",
    "- ```IMG ID```<br>\n",
    "- ```KEGG ID``` <br>\n",
    "- ```User-Supplied ID```\n",
    "\n",
    "> Please fill this file in with your genomes' information. Here is an example file: <br>\n",
    "<img src=\"./Images/Input - Genome Information File.png\" style=\"width: 80%;\">\n",
    "\n",
    ">Some tips:\n",
    ">- Each genome should have its own row. <br><br>\n",
    ">- For ```Organism Type```, the options are: ```Host```,  ```Obligate Symbiont```, or ```Facultative Symbiont```.<br><br>\n",
    ">- Only fill in the IDs for the databases your genome is entered in. Leave the rest of the cells empty. <br><i>(For example, one genome may  be in NCBI and IMG, or it may only in IMG, or it may in all databases.)</i>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='2IMG'></a>\n",
    "### 2) For any genomes in the IMG database\n",
    "\n",
    "> Obtaining genome annotation from the IMG database requires one input file that must be manually downloaded. To get this file, please follow these steps (outlined in the diagram below): <br><br>\n",
    "**1)** Go to the genome entry in IMG. <br>\n",
    "**2)** Scroll down and click on the **\"Export Gene Information\"** button.  <br>\n",
    "**3)** In the page that opens, directly above the table will be a **\"Downloads\"** button.  <br>\n",
    "**4)** Click the highlighted link to download the file. <br>\n",
    "**5)** In the **\"Input\"** folder, you will find another folder called **\"IMG Files\"**. Place the downloaded file from step 4 in there. <br>\n",
    "**6)** Repeat steps 1-5 for all genomes in the IMG database. <br>\n",
    "\n",
    "<img src=\"Images/Input - IMG Files.svg\" style=\"width: 50%;\">\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='3NCBI'></a>\n",
    "### 3) For any genomes in the NCBI database\n",
    "\n",
    "> Obtaining genome annotation from the NCBI database requires several manual steps. This notebook relies on a specific form of genome annotation in which each gene is assigned to a KEGG Orthology number <sup>6</sup> (a.k.a. **KO number**). <br><br>\n",
    "There are no internal links between NCBI genes and KO numbers, (unless the organism is already entered in KEGG, in which case we would simply use the KEGG implementation). Therefore, we need to use a tool called BlastKOALA <sup>7</sup> to do this KO assignment.<br><br>\n",
    "To do this, please follow these steps (also outlined in diagrams below): <br>\n",
    "\n",
    ">**1) Obtaining the organism's entire amino acid sequence in FASTA format:**<br>\n",
    ">>a) Go to the NCBI Assembly page for your genome.<br>\n",
    "b) On the right-hand side, there is a link for the **\"FTP directory for RefSeq assembly\"**. Click this link.<br>\n",
    "c) There will be a directory of links in the page that opens. Select the one that ends in <u><b><span style=\"color:blue;\">protein.faa.gz</span></b></u>; the file will automatically download.<br>\n",
    "<img src=\"Images/Input - NCBI Files 1.svg\" style=\"width: 85%;\">\n",
    "\n",
    ">**2) Submitting a BlastKOALA job:**<br>\n",
    ">>a) Go to https://www.kegg.jp/blastkoala/. <br>\n",
    "b) Under **\"Upload query amino acid sequences in FASTA format\"**, select **\"Choose File\"** and upload your FASTA file from step 1c. <br>\n",
    "c) Choose the taxonomy group and which database to search based on your organism's taxonomic group. <br>\n",
    "d) Enter your email address and select **\"Request for email confirmation\"**.<br>\n",
    "e) You will get an email asking to confirm submission. Select the **\"(Submit)\"** link.<br>\n",
    "\n",
    ">**3) Downloading the BlastKOALA results:**<br>\n",
    ">>a) Once your BlastKOALA job was completed, you will get an email. Open the link in the email.<br>\n",
    "b) Click on the **\"Download\"** button to download the results.<br>\n",
    "c) Rename the downloaded text file as your genome's NCBI ID (e.g., \"ASM960v1.txt\").<br>\n",
    "d) In the **\"Input\"** folder, you will find another folder called **\"NCBI Files\"**. Place the downloaded file from the previous step (c) in there. <br>\n",
    "e) Repeat steps 1-3 for all genomes in the NCBI database. <br>\n",
    "<img src=\"Images/Input - NCBI Files 2.svg\" style=\"width: 70%;\">\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='4User'></a>\n",
    "### 4) For any user-supplied genomes\n",
    "\n",
    "> If you have your own genome annotation that you would like to use, please create a CSV file that meets the following criteria:\n",
    "- Name the file with a unique ID for your genome.<br>\n",
    "- Have only one column in this CSV file named **\"KO Numbers\"**. Fill this column with all of your organism's KO numbers.<br>\n",
    "- In the **\"Input\"** folder, you will find another folder called **\"User-Supplied Files\"**. Place this file in there.<br>\n",
    "<img src=\"Images/Input - User-Supplied Files 1.svg\" style=\"width: 45%;\">\n",
    "\n",
    "> Here is an example file:<br>\n",
    "<img src=\"Images/Input - User-Supplied Files 2.png\" style=\"width: 15%;\">\n",
    "\n",
    "---\n",
    "\n",
    "<b>References / Additional Resources:</b><br>\n",
    "><sup>6</sup> https://www.genome.jp/kegg/ko.html <br>\n",
    "><sup>7</sup> https://www.kegg.jp/blastkoala/ <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "# <span style=\"margin:auto;display:table;\">Thank you! We'll take it from here.</span>\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Setup'></a>\n",
    "## <span style=\"margin:auto;display:table\">Setup</span>\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install libraries\n",
    "\n",
    "To install the BIO.KEGG libraries, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other libraries should come pre-installed with Anaconda. If not, Google ```conda install [library_name]``` and there will be an anaconda.org link with the proper syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge regex \n",
    "#conda install -c anaconda urllib3\n",
    "#conda install -c anaconda requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #Regex string matching\n",
    "import json #JSON serialization & de-serialization\n",
    "import urllib.parse #URL modification\n",
    "import requests #Making HTTP requests\n",
    "from bs4 import BeautifulSoup #Parsing HTML\n",
    "import pandas as pd #Manipulating tabular data\n",
    "pd.options.mode.chained_assignment = None #Gets rid of warning\n",
    "from pprint import pprint # Displays data nicely\n",
    "from Bio.KEGG import REST #KEGG's REST API\n",
    "from Bio.KEGG import KGML #KEGG's KGML API\n",
    "from Bio.KEGG.KGML.KGML_parser import read as kgml_read #Reads KGML pathway map\n",
    "import os #Traverse directory\n",
    "import glob #Grab files that match pattern in directory\n",
    "import math #For determining NaN values\n",
    "from shutil import copyfile #For copying files over\n",
    "import itertools # Creates permutations of organism pairs\n",
    "import networkx as nx # Network visualization\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "import seaborn as sns #Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define auxilary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# Extract the network in tab-delimited edge-notation format\n",
    "def extract_TSV(org_dict):\n",
    "\n",
    "    # Initialize empty list to hold edges\n",
    "    edges = []\n",
    "\n",
    "    # Iterate over categories, pathways, and reactions\n",
    "    for cat_dict in org_dict[\"organism\"][\"categories\"]:\n",
    "        for path_dict in cat_dict[\"pathways\"]:\n",
    "            for reaction_dict in path_dict[\"reactions\"]:\n",
    "                for e in reaction_dict[\"edges\"]:\n",
    "                    if e not in edges:\n",
    "                        edges.append(e) # Append edges to list\n",
    "\n",
    "    # Return list of tab-delimited edges (e.g., [\"C#####\\tC#####\", ...])\n",
    "    return edges\n",
    "\n",
    "#---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='PartA'></a>\n",
    "# <span style=\"margin:auto;display:table;color:blue;\">PART A: Genome Annotation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step1'></a>\n",
    "## <span style=\"margin:auto;display:table\">Step 1: Building the Directory</span>\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required input: \n",
    "> ```genome_information_filepath```  &emsp; *(default = \"./Input/Genome Information.csv\")*<br>\n",
    "\n",
    "#### Overview of process: <br>\n",
    "\n",
    "This notebook was designed to work in conjunction with a specific directory structure (see diagram below).\n",
    "\n",
    "<img src=\"./Images/Directory Structure.svg\" style=\"width: 60%;\">\n",
    "\n",
    "The following code 1) creates this directory structure based on the **\"Genome Information.csv\"** file that the user filled in during the Input section, and 2) returns input that will be used in subsequent steps (e.g., genome information, filepaths for intermediate and final data files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Read in the CSV file of genome information as a pandas DataFrame\n",
    "2) For each genome:\n",
    "    - Create the appropriate folders (as designated in the structural diagram above)\n",
    "    - Return required input for future steps (genome information as well as filepaths)\n",
    "3) Populate and return 3 lists of dictionaries\n",
    "\n",
    "Returns: \n",
    "\n",
    "    Three lists of dictionaries, \n",
    "    one for each organism type (host, obligate symbiont, facultative symbiont)\n",
    "\n",
    "    Each list of dictionaries is in the form:\n",
    "    \n",
    "    [\n",
    "        {\n",
    "        \"database\" : \n",
    "        \"organism_name\" : \n",
    "        \"organism_ID\" : \n",
    "        \"intermediate_filepath\" :\n",
    "        \"destination_filepath_json\" : \n",
    "        \"destination_filepath_csv\" : \n",
    "        \"destination_filepath_csv_nonseeds\" :\n",
    "        \"destination_filepath_pdf\" : \n",
    "        \"input_filepath\" : \n",
    "        \"partA_annotation_complete\" :\n",
    "        \"partB_network_complete\" :\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    Note: \"input_filepath\" will only be returned for genomes in IMG and NCBI\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def step1(genome_information_filepath = \"./Input/Genome Information.csv\"):\n",
    "    \n",
    "    # Read in \"Genome Information.csv\" as a pandas DataFrame\n",
    "    genome_information = pd.read_csv(genome_information_filepath)\n",
    "\n",
    "    # Intialize lists to hold input information\n",
    "    all_inputs = []\n",
    "    \n",
    "    # Iterate over rows in DataFrame\n",
    "    for _, row in genome_information.iterrows():\n",
    "\n",
    "        # Iterate over database columns\n",
    "        for database in [\"KEGG ID\", \"IMG ID\", \"NCBI ID\",\"User-Supplied ID\"]:\n",
    "\n",
    "            # Check that the cell isn't NaN\n",
    "            if not (isinstance(row[database], float) and  math.isnan(row[database])):\n",
    "\n",
    "                # Convert IMG IDs to integers(otherwise a \".0\" will be added to the end)\n",
    "                if database == \"IMG ID\":\n",
    "                    row[database] = int(row[database])\n",
    "                    \n",
    "                # Grab information into new variables (easily readability)\n",
    "                organism_ID = str(row[database])\n",
    "                organism_name = row['Organism Name']\n",
    "                organism_type = row['Organism Type']\n",
    "                database_name = database[:-3]\n",
    "                    \n",
    "                # Define the genome's folderpath\n",
    "                folderpath = \"./Metabolic Networks/\" + organism_type + \"/\" + organism_name + \"/\" + database_name + \" (\" + organism_ID + \")\"\n",
    "                # Define the names of the lowest-level folders \n",
    "                folder_names = [folderpath + \"/Genome Annotation\", folderpath + \"/Metabolic Network\"]\n",
    "                \n",
    "                # Iterate over folder names\n",
    "                for folder_name in folder_names:\n",
    "                    # Recursively make this directory, creating any intermediate directories as needed\n",
    "                    try:\n",
    "                        os.makedirs(folder_name)\n",
    "                    # Don't make anything if the directory already exists\n",
    "                    except FileExistsError:\n",
    "                        pass\n",
    "                  \n",
    "                # Define where we want future intermediate files to go (i.e., in the \"Genome Annotation\" folder for that genome)\n",
    "                intermediate_filepath = folder_names[0] + \"/\" + database_name + \"_\" + organism_ID + \"_KO_numbers.csv\"\n",
    "                \n",
    "                # Define where we want future destination files to go (i.e., in the \"Metabolic Network\" folder for that genome)\n",
    "                destination_filepath_json = folder_names[1] + \"/\" + database_name + \"_\" + organism_ID + \"_metabolic_network.json\"\n",
    "                destination_filepath_csv = folder_names[1] + \"/\" + database_name + \"_\" + organism_ID + \"_metabolic_network.csv\"\n",
    "                destination_filepath_csv_nonseeds = folder_names[1] + \"/\" + database_name + \"_\" + organism_ID + \"_metabolic_network_nonseeds.csv\"\n",
    "                destination_filepath_pdf = folder_names[1] + \"/\" + database_name + \"_\" + organism_ID + \"_metabolic_network.pdf\"\n",
    "                \n",
    "                # Aggregate all of this input into one dictionary\n",
    "                genome_input = {\"database\" : database_name,\\\n",
    "                                \"organism_name\" : organism_name,\\\n",
    "                                \"organism_ID\" : organism_ID,\\\n",
    "                                \"organism_type\" : organism_type,\\\n",
    "                                \"destination_filepath_json\" : destination_filepath_json,\\\n",
    "                                \"destination_filepath_csv\" : destination_filepath_csv,\\\n",
    "                                \"destination_filepath_csv_nonseeds\" : destination_filepath_csv_nonseeds,\\\n",
    "                                \"destination_filepath_pdf\" : destination_filepath_pdf,\\\n",
    "                                \"intermediate_filepath\": intermediate_filepath,\\\n",
    "                                \"partA_annotation_complete\": False,\\\n",
    "                                \"partB_network_complete\": False}\n",
    "                    \n",
    "                #------------------------\n",
    "                # Initialize an indicator variable which will show if the required input files have been supplied\n",
    "                complete_input = True\n",
    "                \n",
    "                # All database sources besides KEGG (i.e., IMG, NCBI, User-supplied) require input files.\n",
    "                # The following section finds those files and copies them into an appropriate location\n",
    "                # in the directory structure.\n",
    "                if database_name != \"KEGG\":\n",
    "                    \n",
    "                    # Find input file for that genome (based on database and ID)\n",
    "                    # For example, an IMG genome should have a corresponding input file in the folder \"IMG Files\"\n",
    "                    input_filepath_source = glob.glob(\"./Input/\" + database_name + \" Files/\" + \"*\" + organism_ID + \"*\")\n",
    "                    \n",
    "                    # If the user didn't input the file properly (i.e., nothing found)\n",
    "                    if input_filepath_source == []:\n",
    "                        complete_input = False # Update indicator variable\n",
    "                        print(\"Error: No \" + database_name + \" File detected for genome ID \" + organism_ID)\n",
    "                        \n",
    "                    # If input file was found\n",
    "                    else:\n",
    "                        \n",
    "                        # For IMG and NCBI, these require an \"input file\" which will be used to generate an \n",
    "                        # \"intermediate file\" after Step 2\n",
    "                        if database_name in [\"IMG\", \"NCBI\"]:\n",
    "                        \n",
    "                            # Define where we want to copy it into\n",
    "                            input_filepath_destination = folder_names[0] + \"/\" + input_filepath_source[0].split(\"/\")[-1]\n",
    "                            # Add to input dictionary\n",
    "                            genome_input[\"input_filepath\"] = input_filepath_destination\n",
    "                            # Copy from \"./Input/[Database] Files\" into appropriate genome's \"./Genome Annotation\" folder\n",
    "                            try:\n",
    "                                copyfile(input_filepath_source[0], input_filepath_destination)\n",
    "                            except FileExistsError:\n",
    "                                print(\"Input file already in correct location.\")\n",
    "                        \n",
    "                        # For user-supplied genomes, these annotations have already been completed, so they don't need\n",
    "                        # to pass through Step 2. Therefore, we just use this \"input file\" as the \"intermediate file\"\n",
    "                        elif database_name == \"User-Supplied\":\n",
    "                            \n",
    "                            # Copy from \"./Input/[Database] Files\" into appropriate genome's \"./Genome Annotation\" folder\n",
    "                            try:\n",
    "                                copyfile(input_filepath_source[0], genome_input[\"intermediate_filepath\"])  \n",
    "                            except FileExistsError:\n",
    "                                print(\"Input file already in correct location.\")\n",
    "                \n",
    "                #------------------------\n",
    "                # Check that this genome hasn't been ran before (check part A & part B separately)\n",
    "                \n",
    "                # Check for complete genome annotations from Part A\n",
    "                if os.path.isfile(genome_input[\"intermediate_filepath\"]) == True:\n",
    "                    genome_input[\"partA_annotation_complete\"] = True\n",
    "                \n",
    "                # Check for complete genome annotations from Part B\n",
    "                if os.path.isfile(genome_input[\"destination_filepath_json\"]) == True:\n",
    "                    genome_input[\"partB_network_complete\"] = True\n",
    "                \n",
    "                #------------------------\n",
    "                # Make sure required input files are present\n",
    "                if complete_input == True:\n",
    "                    \n",
    "                    # Append to list of all genome inputs\n",
    "                    all_inputs.append(genome_input) \n",
    "                    \n",
    "    # Return\n",
    "    return all_inputs\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step2_KEGG'></a>\n",
    "## <span style=\"margin:auto;display:table\">Step 2: If your genome is in KEGG</span>\n",
    "#### Required input: \n",
    "> ```genome_input``` <br>\n",
    "> ```all_KO_names``` <br>\n",
    "> ```all_KO_descriptions```\n",
    "\n",
    "#### Overview of process: <br>\n",
    "The Kyoto Encyclopedia of Genes and Genomes (hereafter called KEGG) maintains an API for public use<sup>1</sup>. We will be accessing that API using the Biopython collection that we installed in the set-up section. In Biopython there is a package called ```Bio.KEGG```; we will use several of its modules in this notebook, but for this step we will only be using the ```REST``` module<sup>2</sup>. \n",
    "\n",
    "First, we use ```REST.kegg_list``` to obtain all available pathways for the inputted organism. Then, we use ```REST.kegg_get``` to obtain the pathway entry file for each pathway, which is returned as plain text<sup>3</sup>. There are two main sections of interest in this file: <b>CLASS</b> and <b>GENE</b>. We check that the first part of the <b>CLASS</b> section is \"Metabolism\" (since we are only concerned with metabolic pathways). Then, for each line in the <b>GENE</b> section, we use regular expressions to extract the gene's KO number. We use the supplied dictionaries, ```all_KO_names``` and ```all_KO_descriptions```, to map relevant information to each gene based on its KO number.\n",
    "\n",
    "\n",
    "This data is printed to a CSV file in the <b>Genome Annotation</b> folder for this genome, using the ```intermediate_filepath``` that was automatically determined in the set-up section. This allows for optional user curation of the KO numbers before proceeding to Step 2.\n",
    "\n",
    "\n",
    "#### References / Additional Resources: <br>\n",
    "> <sup>1</sup> https://www.kegg.jp/kegg/rest/keggapi.html <br>\n",
    "> <sup>2</sup> https://biopython.org/DIST/docs/api/Bio.KEGG.REST-module.html <br>\n",
    "> <sup>3</sup> Chapter 18.2 in http://biopython.org/DIST/docs/tutorial/Tutorial.html \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Use REST.kegg_list to obtain all available pathways for the organism\n",
    "2) Use REST.kegg_get to obtain the pathway entry file for each pathway\n",
    "2) Check that the first part of the CLASS section is \"Metabolism\" \n",
    "   & extract the second part as categoryName\n",
    "3) Use regular expressions to extract the KO number(s) for that pathway\n",
    "4) Print to a CSV file in the \"Genome Annotation\" folder with the following columns:\n",
    "    - \"Database\"\n",
    "    - \"Database Gene ID\"\n",
    "    - \"KO Number\"\n",
    "    - \"KO Name\"\n",
    "    - \"KO Description\"\n",
    "\n",
    "*Note*: Does not return anything, because we \"break\" the pipeline into 2 steps\n",
    "to allow for manual user curation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def step2_KEGG(genome_input, all_KO_names, all_KO_descriptions):\n",
    "    \n",
    "    # Extract organism_ID from the genome_input we found in the setup step\n",
    "    org_ID = genome_input[\"organism_ID\"]\n",
    "\n",
    "    # Initialize DataFrame\n",
    "    # (will be populated with 1 row per gene)\n",
    "    org_genes_df = pd.DataFrame()\n",
    "\n",
    "    # Find length of organism ID -- will be used in string matching\n",
    "    org_ID_len = len(org_ID)\n",
    "\n",
    "    #------------GET ALL PATHWAYS FOR ORGANISM------------\n",
    "\n",
    "    # Get all organism pathways (downloaded as plain text)\n",
    "    pathways_str = REST.kegg_list(\"pathway\", org_ID).read()\n",
    "    \n",
    "    # Convert plain text to set of pathway IDs\n",
    "    pathways_set = set()\n",
    "    for line in pathways_str.rstrip().split(\"\\n\"):\n",
    "        entry, _ = line.split(\"\\t\")\n",
    "        pathways_set.add(entry)\n",
    "\n",
    "    # Convert set to list (set -> list == unique entries only)\n",
    "    pathways = list(pathways_set)\n",
    "\n",
    "    # How the data looks at this stage:\n",
    "    # ['path:org_ID#####', 'path:org_ID#####', 'path:org_ID#####'... ]\n",
    "\n",
    "    #------------FILTER FOR ONLY METABOLIC PATHWAYS------------\n",
    "    \n",
    "    # Iterate over all pathways\n",
    "    for path in pathways:\n",
    "        \n",
    "        # Filter out pathways that start with org_ID011, org_ID012, org_ID010, org_ID07\n",
    "        # (i.e. global, overview, or structural maps without KGML files)\n",
    "        if not path.startswith((\"011\", \"012\", \"010\", \"07\"), org_ID_len + 5):\n",
    "            \n",
    "            # Access KEGG API to get pathway entry file\n",
    "            # (same as https://www.kegg.jp/dbget-bin/www_bget?pathway+org_ID#####)\n",
    "            pathway_file = REST.kegg_get(path).read()\n",
    "\n",
    "            # Extract pathway ID only (i.e. \"path:org_ID#####\" -> \"#####\")\n",
    "            path_index = 5 + org_ID_len\n",
    "            path_ID = path[path_index:]      \n",
    "\n",
    "            # Initialize variable to keep track of file section\n",
    "            current_section = None\n",
    "\n",
    "            # Split the pathway file by newline characters \n",
    "            # For each line, strip any trailing characters to the right of the line\n",
    "            for line in pathway_file.rstrip().split(\"\\n\"):\n",
    "\n",
    "                # Section names are within first 12 characters\n",
    "                section = line[:12].strip()\n",
    "\n",
    "                if not section == \"\":\n",
    "                    # Keep track of the section\n",
    "                    current_section = section\n",
    "\n",
    "                if current_section == \"CLASS\":\n",
    "                    # Category_class will show if it's a metabolic pathway or not\n",
    "                    category_class, category = line[12:].split(\"; \")\n",
    "\n",
    "                if (current_section == \"GENE\"):\n",
    "\n",
    "                    # Find KO section (in case there's another unrelated KO in the description)\n",
    "                    # Should look like [KO:K#####] or [KO:K##### K#####] \n",
    "                    KO_section_regex = re.compile(r\"\\[KO:(K\\d{5})+( *(K\\d{5}))*]\")\n",
    "                    KO_section_search = KO_section_regex.search(line)\n",
    "\n",
    "                    if KO_section_search != None:\n",
    "                        # Within this KO section, grab all KO #s\n",
    "                        start,end = KO_section_search.span()\n",
    "                        KO = re.findall(r\"K\\d{5}\", line[start:end])[0]\n",
    "\n",
    "                        # Grab gene ID (anything before first space)\n",
    "                        database_gene_ID = line[12:].split(\" \")[0]\n",
    "\n",
    "                        # Initialize and populate row\n",
    "                        gene_row = {\"Database\": \"KEGG\",\\\n",
    "                                    \"Database Gene ID\": database_gene_ID,\\\n",
    "                                    \"KO Number\": KO,\\\n",
    "                                    \"KO Name\" : all_KO_names[KO],\\\n",
    "                                    \"KO Description\" : all_KO_descriptions[KO]}\n",
    "\n",
    "                        # Filter for only \"Metabolism\" as Class\n",
    "                        if category_class == \"Metabolism\":\n",
    "\n",
    "                            # Append to DataFrame\n",
    "                            org_genes_df = org_genes_df.append(gene_row, ignore_index = True)\n",
    "                \n",
    "    # Print final DataFrame to file\n",
    "    org_genes_df.to_csv(genome_input[\"intermediate_filepath\"], index = False)\n",
    "\n",
    "    # Print progress\n",
    "    print(\"Step 1 complete.\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step2_IMG'></a>\n",
    "## <span style=\"margin:auto;display:table\">Step 2: If your organism is in IMG</span>\n",
    "#### Required input: \n",
    "> ```genome_input``` <br>\n",
    "> ```all_KO_names``` <br>\n",
    "> ```all_KO_descriptions```\n",
    "\n",
    "#### Overview of process: <br>\n",
    "\n",
    "Unlike KEGG, the Joint Genome Institute's Integrated Microbial Genomes system (hereafter called IMG) does not have an API that allows us to extract KO numbers. Therefore, we will extract this information from the input file the user acquired during the previous [Input](#Input) section. \n",
    "\n",
    "We use this **Input File** to return all genes that have KO annotations, as well as relevant information for each gene (e.g., KO number, Locus Tag, IMG gene ID). We also use the supplied dictionaries, ```all_KO_names``` and ```all_KO_descriptions```, to map relevant information to each gene based on its KO number.\n",
    "\n",
    "This data is printed to a CSV file in the **\"Genome Annotation\"** folder for this genome, using the ```intermediate_filepath``` that was automatically determined in the set-up section. This allows for optional user curation of the KO numbers before proceeding to Step 2.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Read in the XLS file of genome information as a pandas DataFrame\n",
    "2) Filter the DataFrame for KO information only\n",
    "4) Print to a CSV file in the \"Genome Annotation\" folder with the following columns:\n",
    "    - \"Database\"\n",
    "    - \"Database Gene ID\"\n",
    "    - \"Locus Tag\"\n",
    "    - \"KO Number\"\n",
    "    - \"KO Name\"\n",
    "    - \"KO Description\"\n",
    " \n",
    "\n",
    "*Note*: Does not return anything, because we \"break\" the pipeline into 2 steps\n",
    "to allow for manual user curation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def step2_IMG(genome_input, all_KO_names, all_KO_descriptions):\n",
    "    \n",
    "    # Read in as pandas DataFrame\n",
    "    input_df = pd.read_csv(genome_input[\"input_filepath\"], sep = \"\\t\", header = 0, dtype = {'gene_oid':'str'})\n",
    "\n",
    "    # Drop null values (the file is weirdly formatted with blank lines in between)\n",
    "    input_df.dropna(axis = 0, how = \"all\", inplace = True)\n",
    "\n",
    "    # Extract subset of DataFrame with only KO information\n",
    "    ko_df = input_df[[\"gene_oid\", \"Locus Tag\",\"Source\"]].loc[input_df['Source'].str.contains(\"KO:\")]\n",
    "\n",
    "    # Add database column\n",
    "    ko_df[\"Database\"] = \"IMG\"\n",
    "\n",
    "    # Rename columns\n",
    "    ko_df = ko_df.rename(columns={\"gene_oid\":\"Database Gene ID\",\\\n",
    "                                 \"Source\":\"KO Number\"})\n",
    "\n",
    "    # Remove \"KO:\" from \"KO Number\" column\n",
    "    ko_df[\"KO Number\"] = ko_df[\"KO Number\"].map(lambda x: x[3:])\n",
    "    \n",
    "    # Use supplied dictionaries to map KO numbers -> KO names and descriptions\n",
    "    ko_df[\"KO Name\"] = ko_df[\"KO Number\"].apply(lambda x: all_KO_names.get(x))\n",
    "    ko_df[\"KO Description\"] = ko_df[\"KO Number\"].apply(lambda x: all_KO_descriptions.get(x))\n",
    "            \n",
    "    # Print final DataFrame to file\n",
    "    ko_df.to_csv(genome_input[\"intermediate_filepath\"], index = False)\n",
    "\n",
    "    # Print progress\n",
    "    print(\"Step 1 complete.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step2_NCBI'></a>\n",
    "## <span style=\"margin:auto;display:table\">Step 2: If your organism is in NCBI</span>\n",
    "#### Required input: \n",
    "> ```genome_input``` <br>\n",
    "> ```all_KO_names``` <br>\n",
    "> ```all_KO_descriptions```\n",
    "\n",
    "#### Overview of process: <br>\n",
    "\n",
    "Unlike KEGG, the National Center for Biotechnology Information (hereafter called NCBI) does not have an API that allows us to extract KO numbers. Therefore, we will extract this information from the input file the user acquired during the previous [Input](#Input) section. \n",
    "\n",
    "We use this **Input File** to return all genes that have KO annotations, as well as relevant information for each gene (e.g., KO number and Locus Tag). We also use the supplied dictionaries, ```all_KO_names``` and ```all_KO_descriptions```, to map relevant information to each gene based on its KO number.\n",
    "\n",
    "This data is printed to a CSV file in the **\"Genome Annotation\"** folder for this genome, using the ```intermediate_filepath``` that was automatically determined in the set-up section. This allows for optional user curation of the KO numbers before proceeding to Step 2.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Read in the text file of genome information as a pandas DataFrame\n",
    "2) Add column names\n",
    "4) Print to a CSV file in the \"Genome Annotation\" folder with the following columns:\n",
    "    - \"Database\"\n",
    "    - \"Locus Tag\"\n",
    "    - \"KO Number\"\n",
    "    - \"KO Name\"\n",
    "    - \"KO Description\"\n",
    "    \n",
    "*Note*: Does not return anything, because we \"break\" the pipeline into 2 steps\n",
    "to allow for manual user curation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def step2_NCBI(genome_input, all_KO_names, all_KO_descriptions):\n",
    "    \n",
    "    # Read in as pandas DataFrame\n",
    "    input_df = pd.read_csv(genome_input[\"input_filepath\"], sep = \"\\t\", \\\n",
    "                            header = None, names = [\"Locus Tag\", \"KO Number\"])\n",
    "\n",
    "    # Add database column\n",
    "    input_df[\"Database\"] = \"NCBI\"\n",
    "\n",
    "    # Use supplied dictionaries to map KO numbers -> KO names and descriptions\n",
    "    input_df[\"KO Name\"] = input_df[\"KO Number\"].apply(lambda x: all_KO_names.get(x))\n",
    "    input_df[\"KO Description\"] = input_df[\"KO Number\"].apply(lambda x: all_KO_descriptions.get(x))\n",
    "          \n",
    "    # Remove any rows without KO #s\n",
    "    input_df = input_df[input_df[\"KO Number\"].notna()]\n",
    "    \n",
    "    # Print final DataFrame to file\n",
    "    input_df.to_csv(genome_input[\"intermediate_filepath\"], index = False)\n",
    "\n",
    "    # Print progress\n",
    "    print(\"Step 1 complete.\")     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Full_Pipeline_PartA'></a>\n",
    "## <span style=\"margin:auto;display:table;\">Full Pipeline for Part A</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 1-2\n",
    "def full_pipeline_partA():\n",
    "    \n",
    "    # Get genome inputs based on their \n",
    "    all_inputs = step1()\n",
    "    \n",
    "    #------------GET DICTIONARY OF KO NAMES {ID:Name}------------\n",
    "    all_KO_names = {}\n",
    "    all_KO_descriptions = {}\n",
    "\n",
    "    # Preferred method is getting this information directly from the API\n",
    "    try:\n",
    "        orthology_str = REST.kegg_list(\"orthology\").read()\n",
    "        \n",
    "    # However, since the API is not always reliable, we've pre-scraped\n",
    "    # this information into a text file\n",
    "    except:\n",
    "        with open(r\"./Supplementary Materials/Orthology_List.txt\",\"r\") as f:\n",
    "            orthology_str = str(f.read())\n",
    "\n",
    "        for line in orthology_str.rstrip().split(r\"\\n\"):\n",
    "            KO_ID, KO_name_description = line.split(r\"\\t\")\n",
    "\n",
    "            if r\";\" in KO_name_description:\n",
    "                KO_name, KO_description = KO_name_description.split(r\";\", 1)\n",
    "\n",
    "            #One KO annotation is in a different format, no semi-colon \n",
    "            #(\"CCAAT/enhancer binding protein (C/EBP), other\")\n",
    "            else: \n",
    "                KO_name = KO_name_description\n",
    "                KO_description = \"\"\n",
    "\n",
    "            all_KO_names[KO_ID[3:]] = KO_name\n",
    "            all_KO_descriptions[KO_ID[3:]] = KO_description.strip()\n",
    "        \n",
    "    #------------\n",
    "    # Iterate over inputs\n",
    "    for genome_input in all_inputs:\n",
    "        \n",
    "        # Print which genome is currently in progress\n",
    "        print(genome_input[\"organism_name\"], genome_input[\"database\"], genome_input[\"organism_ID\"])\n",
    "        \n",
    "        # Check that the annotation isn't already complete\n",
    "        if genome_input[\"partA_annotation_complete\"] == True:\n",
    "            print(\"Part A already complete for this genome.\")\n",
    "        \n",
    "        else:\n",
    "            # Execute the correct Step 2 script based on the genome's database\n",
    "            if genome_input[\"database\"] == \"KEGG\":\n",
    "                step2_KEGG(genome_input = genome_input, \\\n",
    "                           all_KO_names = all_KO_names, \\\n",
    "                           all_KO_descriptions = all_KO_descriptions)\n",
    "\n",
    "            elif genome_input[\"database\"] == \"IMG\":\n",
    "                step2_IMG(genome_input = genome_input, \\\n",
    "                           all_KO_names = all_KO_names, \\\n",
    "                           all_KO_descriptions = all_KO_descriptions)\n",
    "\n",
    "            elif genome_input[\"database\"] == \"NCBI\":\n",
    "                step2_NCBI(genome_input = genome_input, \\\n",
    "                           all_KO_names = all_KO_names, \\\n",
    "                           all_KO_descriptions = all_KO_descriptions)\n",
    "            \n",
    "    # Print progress\n",
    "    print(\"Part A of Full Pipeline Complete\")\n",
    "    \n",
    "    # Return all_inputs for part 2\n",
    "    return all_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acyrthosiphon pisum KEGG api\n",
      "Part A already complete for this genome.\n",
      "Buchnera aphidicola Tuc7 NCBI ASM2106v1\n",
      "Part A already complete for this genome.\n",
      "Buchnera aphidicola 5A NCBI ASM2108v1\n",
      "Part A already complete for this genome.\n",
      "Buchnera aphidicola APS NCBI ASM960v1\n",
      "Part A already complete for this genome.\n",
      "Regiella insecticola LSR1 NCBI ASM14362v1\n",
      "Part A already complete for this genome.\n",
      "Rickettsiella viridis Ap-RA04 NCBI ASM396675v1\n",
      "Part A already complete for this genome.\n",
      "Serratia symbiotica Tuscon NCBI ASM18648v1\n",
      "Part A already complete for this genome.\n",
      "Fukatsuia symbiotica 5D NCBI ASM312242v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa 5AT (AA) NCBI ASM2170v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa NY26 (AA) NCBI New_ASM277729v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa MI47 (BB) NCBI New_ASM226940v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa 5D (BB) NCBI New_ASM312244v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa MI12 (CC) NCBI New_ASM359054v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa A2C (DD) NCBI New_ASM277719v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa AS3 (DD) NCBI New_ASM277721v1\n",
      "Part A already complete for this genome.\n",
      "Hamiltonella defensa ZA17 (EE) NCBI New_ASM277723v1\n",
      "Part A already complete for this genome.\n",
      "Part A of Full Pipeline Complete\n",
      "CPU times: user 31.4 ms, sys: 22.7 ms, total: 54.1 ms\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%time all_inputs = full_pipeline_partA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='PartB'></a>\n",
    "# <span style=\"margin:auto;display:table;color:blue;\">PART B: Network Reconstruction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step3'></a>\n",
    "## <span style=\"margin:auto;display:table\">Step 3: Filtering and Extracting Pathway Data</span>\n",
    "\n",
    "\n",
    "#### Required input: \n",
    ">```genome_input``` <br>\n",
    ">```all_compound_names```\n",
    "\n",
    "#### Overview of process:\n",
    "For Step 3, we add in reaction-level and compound-level data to the output from Step 2. To do this, we're going to access KEGG Markup Language (KGML) files <sup>1</sup> from the KEGG API. These XML-style files describe the network graphs that you see on the KEGG website. \n",
    "We retrieve these files using ```REST.kegg_get``` and parse them using ```KGML_parser.read```.\n",
    "\n",
    "It's important to note that there are different prefixes for KEGG pathways <sup>2</sup>. The number 00010 represents \"Glycolysis / Gluconeogenesis\", but what's in front changes the contents of the KGML file. For example, putting an organism ID in front (i.e. \"hde00010\") makes it specific to <i>Hamiltonella defensa 5AT</i>, highlighting only the KO numbers that organism has genes for. However, putting a \"ko\" in front (i.e. \"ko00010\") makes it non-specific to any organism, and highlights all genes.\n",
    "\n",
    "By accessing the non-specific \"ko\" KGML file for each pathway, we can then filter it by the KO numbers we retrieved in Step 1. We retrieve data for each reaction and its compounds. We also include edge-notation format for each reaction, to be used in Step 4. The final dictionary is updated and returned as output.\n",
    "\n",
    "**A quick note:** Downloading the organism-specific file for an organism already in KEGG gives you the same results as downloading the non-specific file and filtering it by KO number. The reason for breaking up these steps is so that the code is modularized to work with data sources other than KEGG, including IMG and NCBI. \n",
    "\n",
    "---\n",
    "\n",
    " #### References / Additional Resources: <br>\n",
    "\n",
    "><sup>1</sup> https://www.kegg.jp/kegg/xml/docs/ <br>\n",
    "><sup>2</sup> Pathway Identifiers section https://www.kegg.jp/kegg/pathway.html <br>\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) For each pathway, retrieve the non-specific KGML file using REST.kegg_get\n",
    "2) Use the output from Step 2 to find the KO numbers for each pathway,\n",
    "    filter the KGML file using those\n",
    "3) Extract reaction and compound data\n",
    "4) Format reactions into edge-notation\n",
    "5) Populate and return dictionary\n",
    "\n",
    "Returns: a dictionary in the form \n",
    "    {\n",
    "        \"organism\": {\n",
    "            \"organismID\" :\n",
    "            \"organismName\" :\n",
    "            \"organismSource\" :\n",
    "            \"organismType\" :\n",
    "            \"categories\" : [\n",
    "                {\n",
    "                \"categoryName\" :\n",
    "                \"pathways\" : [\n",
    "                    {\n",
    "                    \"pathwayID\" :\n",
    "                    \"pathwayName\" :\n",
    "                    \"allKOnumbers\" :\n",
    "                    \"reactions\" : [\n",
    "                        {\n",
    "                        \"reactionID\" :\n",
    "                        \"reactionType\" :\n",
    "                        \"KOnumbers\" :\n",
    "                        \"edges\" :\n",
    "                        \"compounds\" : [\n",
    "                            {\n",
    "                            \"compoundID\" :\n",
    "                            \"compoundType\" :\n",
    "                            \"compoundName\":\n",
    "                            }\n",
    "                            ,...\n",
    "                        ]\n",
    "                        }\n",
    "                        ,...\n",
    "                    ]\n",
    "                    }\n",
    "                    ,...\n",
    "                ]\n",
    "                }\n",
    "                ,...\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def step3(genome_input, all_compound_names):\n",
    "        \n",
    "    #------------------------\n",
    "    # Extract edge-notation format given the substrates and products\n",
    "    # (i.e., \"[substrate\\tproduct, ...]\")\n",
    "    def edge_notation(sub_lst, prod_lst): #tab-separated\n",
    "\n",
    "        # Initialize list of edges\n",
    "        edge_lst = []\n",
    "        \n",
    "        # Iterate over substrates\n",
    "        for sub in sub_lst:\n",
    "            # Iterate over products\n",
    "            for prod in prod_lst:\n",
    "\n",
    "                # Create reaction in edge-notation (\"substrate\\tproduct\")\n",
    "                line = sub + \"\\t\" + prod\n",
    "                # Append to edges list\n",
    "                edge_lst.append(line)\n",
    "\n",
    "        # Return\n",
    "        return edge_lst\n",
    "    \n",
    "    #------------------------\n",
    "    # Read in file from Step 2\n",
    "    intermediate_file = pd.read_csv(genome_input[\"intermediate_filepath\"])\n",
    "    \n",
    "    # Grab all of organism's KO numbers from Step 2 file\n",
    "    org_KO_numbers = intermediate_file[\"KO Number\"].tolist()\n",
    "    \n",
    "    # Initialize the main dictionary\n",
    "    org_dict = {}\n",
    "    # Establish root element\n",
    "    org_dict[\"organism\"] = {} \n",
    "    # Fill in organism descriptors\n",
    "    org_dict[\"organism\"][\"organismID\"] = genome_input[\"organism_ID\"]\n",
    "    org_dict[\"organism\"][\"organismName\"] = genome_input[\"organism_name\"]\n",
    "    org_dict[\"organism\"][\"organismSource\"] = genome_input[\"database\"]\n",
    "    org_dict[\"organism\"][\"organismType\"] = genome_input[\"organism_type\"]\n",
    "    \n",
    "    # Explicitly define category-pathway structure for consistency \n",
    "    org_dict[\"organism\"][\"categories\"] = [{\"categoryName\" : \"Carbohydrate metabolism\", \\\n",
    "                                               \"pathwayIDs\" : [\"00010\", \"00020\", \"00030\", \"00040\", \"00051\", \"00052\", \"00053\", \"00500\", \"00520\", \"00620\", \"00630\", \"00640\", \"00650\", \"00660\", \"00562\"]}, \\\n",
    "                                          {\"categoryName\" : \"Energy metabolism\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00190\", \"00195\", \"00196\", \"00710\", \"00720\", \"00680\", \"00910\", \"00920\"]}, \\\n",
    "                                          {\"categoryName\" : \"Lipid metabolism\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00061\", \"00062\", \"00071\", \"00073\", \"00100\", \"00120\", \"00121\", \"00140\", \"00561\", \"00564\", \"00565\", \"00600\", \"00590\", \"00591\", \"00592\", \"01040\"]}, #\"00072\" was removeds\n",
    "                                          {\"categoryName\" : \"Nucleotide metabolism\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00230\", \"00240\"]}, \\\n",
    "                                          {\"categoryName\" : \"Amino acid metabolism\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00250\", \"00260\", \"00270\", \"00280\", \"00290\", \"00300\", \"00310\", \"00220\", \"00330\", \"00340\", \"00350\", \"00360\", \"00380\", \"00400\"]}, \\\n",
    "                                          {\"categoryName\" : \"Metabolism of other amino acids\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00410\", \"00430\", \"00440\", \"00450\", \"00460\", \"00470\", \"00480\"]}, #\"00471\",\"00472\", \"00473\" removed and \"00470\" added\n",
    "                                          {\"categoryName\" : \"Glycan biosynthesis and metabolism\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00510\", \"00513\", \"00512\", \"00515\", \"00514\", \"00532\", \"00534\", \"00533\", \"00531\", \"00563\", \"00601\", \"00603\", \"00604\", \"00540\", \"00542\", \"00541\", \"00550\", \"00511\", \"00571\", \"00572\"]}, # added \"00542\"\n",
    "                                          {\"categoryName\" : \"Metabolism of cofactors and vitamins\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00730\", \"00740\", \"00750\", \"00760\", \"00770\", \"00780\", \"00785\", \"00790\", \"00670\", \"00830\", \"00860\", \"00130\"]}, \\\n",
    "                                          {\"categoryName\" : \"Metabolism of terpenoids and polyketides\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00900\", \"00902\", \"00909\", \"00904\", \"00906\", \"00905\", \"00981\", \"00908\", \"00903\", \"00281\", \"01052\", \"00522\", \"01051\", \"01059\", \"01056\", \"01057\", \"00253\", \"00523\", \"01054\", \"01053\", \"01055\"]}, \\\n",
    "                                          {\"categoryName\" : \"Biosynthesis of other secondary metabolites\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00940\", \"00945\", \"00941\", \"00944\", \"00942\", \"00943\", \"00901\", \"00403\", \"00950\", \"00960\", \"01058\", \"00232\", \"00965\", \"00966\", \"00402\", \"00311\", \"00332\", \"00261\", \"00331\", \"00521\", \"00524\", \"00525\", \"00401\", \"00404\", \"00405\", \"00333\", \"00254\", \"00999\", \"00998\", \"00997\"]}, \\\n",
    "                                          {\"categoryName\" : \"Xenobiotics biodegradation and metabolism\", \\\n",
    "                                               \"pathwayIDs\" :  [\"00362\", \"00627\", \"00364\", \"00625\", \"00361\", \"00623\", \"00622\", \"00633\", \"00642\", \"00643\", \"00791\", \"00930\", \"00363\", \"00621\", \"00626\", \"00624\", \"00365\", \"00984\", \"00980\", \"00982\", \"00983\"]}]\n",
    "\n",
    "    #------------------------          \n",
    "    # Iterate over categories\n",
    "    for cat_dict in org_dict[\"organism\"][\"categories\"]:\n",
    "        \n",
    "        # Initialize list to hold pathway dictionaries\n",
    "        # (Will be converting the list of pathway IDs into a list of pathway dictionaries)\n",
    "        cat_dict[\"pathways\"] = []\n",
    "        \n",
    "        # Iterate over pathway_IDs\n",
    "        for path_ID in cat_dict[\"pathwayIDs\"]:\n",
    "            \n",
    "            # Initialize pathway dictionary\n",
    "            path_dict = {}\n",
    "            path_dict[\"pathwayID\"] = path_ID \n",
    "            \n",
    "            # Initialize sets that will hold all KO numbers and reactions within\n",
    "            # this pathway that the organism is capable of\n",
    "            path_dict[\"allKOnumbers\"] = set()\n",
    "            path_dict[\"reactions\"] = []\n",
    "            \n",
    "            # Get KGML file for whole pathway\n",
    "            KO_path = \"ko\" + str(path_ID)\n",
    "            \n",
    "            #print(KO_path)\n",
    "            \n",
    "            KGML_file = REST.kegg_get(KO_path, \"kgml\").read()\n",
    "            \n",
    "            # Extract pathway name from KGML file\n",
    "            path_dict[\"pathwayName\"] = kgml_read(KGML_file).title\n",
    "\n",
    "            # Grab reaction_entries from KGML file\n",
    "            reaction_entries = kgml_read(KGML_file).reaction_entries\n",
    "            \n",
    "            # Initialize a dict of node_ids to be filled with any reaction entry\n",
    "            # that the organism is capable of (i.e., has a KO number for)\n",
    "            org_node_IDs = {} \n",
    "\n",
    "            # Iterate over reaction_entries\n",
    "            for k in reaction_entries:\n",
    "\n",
    "                # Grab kos per reaction_entry\n",
    "                KGML_KOs = [i[3:] for i in re.findall(r\"ko:K\\d{5}\", k.name)]\n",
    "\n",
    "                # If there are any KO #s in common\n",
    "                if any(x in KGML_KOs for x in org_KO_numbers):\n",
    "                    # Grab them\n",
    "                    both = [KO for KO in KGML_KOs if KO in org_KO_numbers]\n",
    "                    \n",
    "                    # Add to dictionary\n",
    "                    org_node_IDs[k.id] = both\n",
    "\n",
    "            # Grab all reactions in pathway from KGML file\n",
    "            reactions = kgml_read(KGML_file).reactions\n",
    "\n",
    "            # Iterate over reactions\n",
    "            for k in reactions:\n",
    "                \n",
    "                # Grab the ones that match the organism's reaction entry\n",
    "                if k.id in org_node_IDs.keys():\n",
    "\n",
    "                    # Grab all substrate entries\n",
    "                    substrates1 = [s.name for s in k.substrates]\n",
    "                    # Make sure they fit the right structure (i.e., C##### or G#####)\n",
    "                    substrates = [i for s in substrates1 for i in re.findall(r'C\\d{5}|G\\d{5}', s)]\n",
    "                    \n",
    "                    # Grab all product entries\n",
    "                    products1 = [s.name for s in k.products]\n",
    "                    # Make sure they fit the right structure (i.e., C##### or G#####)\n",
    "                    products = [i for s in products1 for i in re.findall(r'C\\d{5}|G\\d{5}', s)]\n",
    "\n",
    "                    # Convert to uni-directional/ one-way reactions to tab-delimited edge notation\n",
    "                    if k.type == 'irreversible': \n",
    "                        edge_lst = edge_notation(substrates, products)\n",
    "                        \n",
    "                    # Convert to bi-directional reactions to tab-delimited edge notation\n",
    "                    elif k.type == 'reversible': \n",
    "                        edge_lst1 = edge_notation(substrates, products)\n",
    "                        edge_lst2 = edge_notation(products, substrates)\n",
    "                        edge_lst = edge_lst1 + edge_lst2\n",
    "\n",
    "                    # Initialize and populate reaction dictionary\n",
    "                    reaction_dict = {}\n",
    "                    reaction_dict[\"reactionID\"] = k.id\n",
    "                    reaction_dict[\"reactionType\"] = k.type \n",
    "                    reaction_dict[\"edges\"] = edge_lst\n",
    "                    reaction_dict[\"KOnumbers\"] = org_node_IDs[k.id]\n",
    "                    reaction_dict[\"compounds\"] = []\n",
    "                    \n",
    "                    # Add reaction KOnumbers to full set of KO numbers for pathway\n",
    "                    path_dict[\"allKOnumbers\"].update(org_node_IDs[k.id])\n",
    "                    \n",
    "                    # Initialize and populate compound dictionaries\n",
    "                    for sub in substrates:\n",
    "                        \n",
    "                        compound_dict = {\"compoundID\": sub,\\\n",
    "                                        \"compoundType\": \"substrate\",\\\n",
    "                                        \"compoundName\": all_compound_names[sub]}\n",
    "                        \n",
    "                        # Add to reaction dictionary\n",
    "                        reaction_dict[\"compounds\"].append(compound_dict)\n",
    "                    \n",
    "                    # Initialize and populate compound dictionaries\n",
    "                    for prod in products:\n",
    "                        compound_dict = {\"compoundID\": prod,\\\n",
    "                                        \"compoundType\": \"product\",\\\n",
    "                                        \"compoundName\": all_compound_names[prod]}\n",
    "                        \n",
    "                        # Add to reaction dictionary\n",
    "                        reaction_dict[\"compounds\"].append(compound_dict)\n",
    "                    \n",
    "                    # Add reaction dictionary to pathway dictionary\n",
    "                    path_dict[\"reactions\"].append(reaction_dict)\n",
    "\n",
    "            # Convert set to list (set -> list == unique entries only)\n",
    "            path_dict[\"allKOnumbers\"] = list(path_dict[\"allKOnumbers\"])\n",
    "            \n",
    "            # Add pathway dictionary to category dictionary\n",
    "            cat_dict[\"pathways\"].append(path_dict)\n",
    "            \n",
    "        # Now that we have the populated dictionaries, remove the list of IDs\n",
    "        del cat_dict[\"pathwayIDs\"]\n",
    "            \n",
    "    # Print progress\n",
    "    print(\"Step 3 complete.\")\n",
    "    \n",
    "    # Return current data structure\n",
    "    return org_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "<a id='Step4'></a>\n",
    "## <span style=\"margin:auto;display:table\">Step 4: Determing Seeds/Nonseeds and Children/Parents</span>\n",
    "\n",
    "#### Required input: \n",
    "> Output from Step 3 <br>\n",
    "> ```max_component_size``` &emsp; *(default = 5)*<br>\n",
    "> ```distance_transformation``` &emsp; *(default = \"1/x\")*<br>\n",
    "> ```weight_measure``` &emsp; *(default = \"Degree Centrality\")*<br>\n",
    "                        \n",
    "\n",
    "#### Overview of process:\n",
    "\n",
    "As mentioned before, this pipeline is specifically designed to be run in conjunction with the  **NetInteract** tool, which is inspired by the Borenstein lab's NetCooperate tool <sup>4,5</sup>. \n",
    "\n",
    "##### SEEDS vs. NON-SEEDS:\n",
    "\n",
    "The original NetCooperate tool requires seed information to compute complementarity and support scores. <span style=\"color:red;\">\"Seeds\"</span> are compounds that are exogenously acquired, while <span style=\"color:blue;\">\"non-seeds\"</span> are endogenously acquired. In simpler terms, this means that <span style=\"color:red;\">seeds</span> are the outside input into a metabolic network, while <span style=\"color:blue;\">non-seeds</span> are the inside capacity of that network. An organism requires <span style=\"color:red;\">seeds</span> in order to produce <span style=\"color:blue;\">non-seeds</span>. (See the below figure for a visual representation.)\n",
    "\n",
    "<img src=\"./Images/Step 4 - Seeds and Nonseeds.svg\" style=\"width: 60%;\">\n",
    "\n",
    "In order to distinguish <span style=\"color:red;\">seeds</span> from <span style=\"color:blue;\">non-seeds</span>, we use network topology. (<i>Theory borrowed from the Borenstein lab's methods<sup>6</sup>. Code inspired by the tool PhyloMint <sup>7,8</sup></i>.) \n",
    "    \n",
    "Here is a simplifed explanation of the process:\n",
    "> **1)** The nested data structure is first converted into edge-notation and then into a directed graph. <br><br>\n",
    "> **2)** Then, the graph's strongly connected components (SCCs) are determined using Networkx's implementation of Kosaraju's algorithm<sup>9</sup>. <br><br>\n",
    "> **3)** SCCs that do NOT have any incoming edges are classified as <span style=\"color:red;\">seed groups</span>, such that all compounds inside of the SCC have an equal chance of being a true <span style=\"color:red;\">seed</span>. Compounds within SCCs with at least one incoming edge are classified as <span style=\"color:blue;\">non-seeds</span>.<br>\n",
    "   \n",
    "Using this method, we can distinguish whether each compound is a <span style=\"color:red;\">seed</span> or <span style=\"color:blue;\">non-seed</span>. <br>\n",
    "\n",
    "Additionally, for each <span style=\"color:blue;\">non-seed</span>, we find its “weight” using degree centrality. This is a common metric for finding the importance of nodes in a network and is defined as the number of incoming and outgoing reactions from that node. <br>\n",
    "\n",
    "##### PARENT / CHILD RELATIONSHIPS:\n",
    "   \n",
    "Since you can think of <span style=\"color:blue;\">non-seeds</span> as being in the center of the network and the <span style=\"color:red;\">seeds</span> as being on the outside, the parent <span style=\"color:red;\">seed groups</span> of a <span style=\"color:blue;\">non-seed</span> are the ones that eventually trickle down the network to that <span style=\"color:blue;\">non-seed</span> (hence, “parent”). One <span style=\"color:blue;\">non-seed</span> can have multiple parent <span style=\"color:red;\">seed groups</span>, and one <span style=\"color:red;\">seed group</span> can be the parent of many <span style=\"color:blue;\">non-seeds</span>, so it’s not a one-to-one comparison. <br>\n",
    "\n",
    "The original NetCooperate tool does not consider the downstream metabolic effects of each <span style=\"color:red;\">seed</span>, considering them all of equal importance. However, this is not necessarily true. One <span style=\"color:red;\">seed</span> may go on to impact half of the entire network of <span style=\"color:blue;\">non-seeds</span>, while another may only impact a handful of <span style=\"color:blue;\">non-seeds</span>. <br>\n",
    "\n",
    "To account for this variation in <span style=\"color:red;\">seed</span> impact, we've implemented the additional step of finding relationships between metabolites. For each \"parent\" <span style=\"color:red;\">seed</span>, we find all of the <span style=\"color:blue;\">non-seeds</span> that are its \"children\", as well as the distance (in # of reactions) between these compounds. (We theorized a seed group that is directly connected to a nonseed would have more impact than one that is 25 reactions away.) This information will be used in the **NetInteract** tool to calibrate interaction scores based on their predicted downstream metabolic effects.\n",
    "\n",
    "The final dictionary is populated and returned as output.\n",
    "\n",
    "---\n",
    "\n",
    "<b>References / Additional Resources:</b><br>\n",
    "><sup>4</sup> https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0588-y <br>\n",
    "><sup>5</sup> http://depts.washington.edu/elbogs/NetCooperate/NetCooperateWeb.cgi <br>\n",
    "><sup>6</sup> https://www.pnas.org/content/suppl/2008/09/11/0806162105.DCSupplemental/0806162105SI.pdf#nameddest=STXT <br>\n",
    "><sup>7</sup> https://github.com/mgtools/PhyloMint <br>\n",
    "><sup>8</sup> https://pubmed.ncbi.nlm.nih.gov/33125363/<br>\n",
    "><sup>9</sup> https://networkx.org/documentation/networkx-1.9/reference/generated/networkx.algorithms.components.strongly_connected.strongly_connected_components.html\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Extract edge-notation file from nested data structure\n",
    "2) Convert to graph\n",
    "3) Use Borenstein lab's implementation of the Kosaraju algorithm\n",
    "    to determine seeds, seedgroups, nonseeds, and allnodes\n",
    "4) Determine seed \"parents\" (predecessors) and nonseed \"children\" (successors)\n",
    "5) Update the data structure with seed information\n",
    "\n",
    "Returns: a dictionary in the form \n",
    "    {\n",
    "        \"organism\": {\n",
    "            \"organismID\" :\n",
    "            \"organismName\" :\n",
    "            \"organismSource\" : \n",
    "            \"organismType\"\n",
    "            \"categories\" : [\n",
    "                {\n",
    "                \"categoryName\" :\n",
    "                \"pathways\" : [\n",
    "                    {\n",
    "                    \"pathwayID\" :\n",
    "                    \"pathwayName\" :\n",
    "                    \"allKOnumbers\" :\n",
    "                    \"reactions\" : [\n",
    "                        {\n",
    "                        \"reactionID\" :\n",
    "                        \"reactionType\" :\n",
    "                        \"KOnumbers\" :\n",
    "                        \"edges\" :\n",
    "                        \"compounds\" : [\n",
    "                            {\n",
    "                            \"compoundID\" :\n",
    "                            \"compoundType\" :\n",
    "                            \"compoundName\"\n",
    "                            \"isSeed\" :\n",
    "                            \"seedGroupID\" :\n",
    "                            }\n",
    "                            ,...\n",
    "                        ]\n",
    "                        }\n",
    "                        ,...\n",
    "                    ]\n",
    "                    }\n",
    "                    ,...\n",
    "                ]\n",
    "                }\n",
    "                ,...\n",
    "            ]\n",
    "            \"allNodes\" : []\n",
    "            \"allSeeds\" : []\n",
    "            \"allSeedGroups\" : [\n",
    "                {\n",
    "                \"seedGroupID\":\n",
    "                \"seedCompoundIDs\": []\n",
    "                \"nonSeedSuccessors\": [\n",
    "                    {\n",
    "                    \"compoundID\" :\n",
    "                    \"distance\" :\n",
    "                    \"weight\" :\n",
    "                    }\n",
    "                    ]\n",
    "                \"networkImpact\" : \n",
    "                    {\n",
    "                    \"total\" : \n",
    "                        {\n",
    "                        \"score\" :\n",
    "                        }\n",
    "                    \"unique\" : \n",
    "                        {\n",
    "                        \"score\" :\n",
    "                        \"nonSeedIDs\" : []\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                ,...\n",
    "            ]\n",
    "            \"allNonSeeds\" : [\n",
    "                {\n",
    "                \"compoundID\": \n",
    "                \"weight\":\n",
    "                \"seedGroupPredecessors\": [\n",
    "                    {\n",
    "                    \"seedGroupID\" :\n",
    "                    \"distance\" :\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def step4(org_dict, max_component_size = 5, distance_transformation = \"1/x\", \\\n",
    "                        weight_measure = \"Degree Centrality\"):\n",
    "\n",
    "    \n",
    "    #---------------------------------------------------------\n",
    "    # Based on a seed and the strongly connected component it's in, \n",
    "    # return all nonseed successors\n",
    "    def find_successors(seed, SCC, graph, distance_transformation, all_node_weights):\n",
    "        \n",
    "        # Find the successors from that seed\n",
    "        successors_temp = nx.dfs_successors(graph, seed)\n",
    "        # In the dfs_successors object, successors are stored as keys AND values -- grab all\n",
    "        successors_keys = [k for k in successors_temp.keys() if k not in SCC]\n",
    "        successors_values = [i for v in successors_temp.values() for i in v if i not in SCC]\n",
    "        successors_all = list(set(successors_keys + successors_values))\n",
    "\n",
    "        successors_list = [] # Will append a dictionary for each successor node\n",
    "        # For each of the successors, find the shortest path between them\n",
    "        for successor in successors_all:\n",
    "            shortest_path_length = nx.shortest_path_length(graph, source = seed, target = successor)\n",
    "\n",
    "            # By scaling this way, successors are weighted less if they are further away from the seed\n",
    "            if distance_transformation == \"1/x\":\n",
    "                scaled_distance = 1/shortest_path_length\n",
    "\n",
    "            # Find weight of that specific successor\n",
    "            successor_weight = all_node_weights[successor]\n",
    "\n",
    "            successor_tempdict = {\"compoundID\" : successor,\\\n",
    "                                 \"distance\": scaled_distance,\\\n",
    "                                 \"weight\" : successor_weight}\n",
    "\n",
    "            successors_list.append(successor_tempdict)\n",
    "\n",
    "        return successors_list\n",
    "        \n",
    "    #---------------------------------------------------------\n",
    "    # Inspired by PhyloMint's BuildGraphNetX.py script\n",
    "    # https://github.com/mgtools/PhyloMint\n",
    "    \n",
    "    # Improvements by PhyloMint:\n",
    "    # Improves upon NetCooperate module implementation which erroneously discards \n",
    "    # certain cases of SCCs (where a smaller potential SCC lies within a larger SCC)\n",
    "\n",
    "    # Improvements here:\n",
    "    # FIX DESCRIPTION!\n",
    "    # - Still uses nodes in SCCs > maxComponentSize by breaking them up into SCCs of size 1\n",
    "    # - Adds in maximum number of successors from seed group (i.e., downstream impact \n",
    "    # of that node on the network), as cited in the NetCmpt literature\n",
    "    # - Also scales these successors based on network centrality measures and distance from seed\n",
    "    # input (so closer and more important nonseeds are weighted higher)\n",
    "    \n",
    "    # Note: Decided to keep all seed information in the allSeedGroups variable \n",
    "    # instead of for each seed in the compounds variable.\n",
    "    \n",
    "    def calculate_seeds(edges, max_component_size = 5, distance_transformation = \"1/x\", \\\n",
    "                        weight_measure = \"Degree Centrality\"):\n",
    "        \n",
    "        # Construct directed graph from edge-notation format\n",
    "        graph = nx.parse_edgelist(edges, delimiter='\\t', create_using = nx.DiGraph())\n",
    "\n",
    "        # Get strongly connected components (SCC)\n",
    "        SCCs = nx.strongly_connected_components(graph)\n",
    "        \n",
    "        # Grab weight scores for entire network\n",
    "        if weight_measure == \"Degree Centrality\":\n",
    "            all_node_weights = nx.degree_centrality(graph)\n",
    "\n",
    "        #------------INTIALIZE SOME VARIABLES------------\n",
    "        \n",
    "        # Initialize all_nodes, all_nonseeds, and all_seeds\n",
    "        all_nodes = list(graph.nodes()) # List of compound IDs\n",
    "        all_seeds = [] # Will become list of seed compounds IDs\n",
    "        \n",
    "        # Initialize all_seedgroups\n",
    "        # Will become a list of dictionaries in the form:\n",
    "        #    [\n",
    "        #        {\n",
    "        #        \"seedGroupID\":\n",
    "        #        \"seedCompoundIDs\": []\n",
    "        #        \"nonSeedSuccessors\": [\n",
    "        #            {\n",
    "        #            \"compoundID\" :\n",
    "        #            \"distance\" :\n",
    "        #            \"weight\" :\n",
    "        #            }\n",
    "        #        ]\n",
    "        #        \"networkImpactTotal\":\n",
    "        #        \"networkImpactUnique\": \n",
    "        #        }\n",
    "        #        ,...\n",
    "        #    ]\n",
    "        all_seedgroups = []\n",
    "        seedgroup_ID = 0 # Will be incremented\n",
    "        \n",
    "        \n",
    "        #------------ITERATE OVER SCCs IN NETWORK TO FIND SEEDGROUPS------------\n",
    "        # Note: First we find seedgroups, then we find nonseeds, then we find seedgroup impact.\n",
    "        # We have to do this in stages because if we start finding the nonseed predecessor seed groups before\n",
    "        # all seed groups have been found, we'll have incomplete results.\n",
    "        \n",
    "        # For each SCC in the network\n",
    "        for SCC in SCCs:\n",
    "            \n",
    "            # Convert to list\n",
    "            SCC_lst = list(SCC)\n",
    "            \n",
    "            #------------IF SCC SIZE > Max SIZE------------\n",
    "            \n",
    "            # Filter out SCC larger than threshold\n",
    "            # Check each individual node for seed/nonseed distinction\n",
    "            if len(SCC_lst) > max_component_size:\n",
    "                \n",
    "                for node in SCC_lst:\n",
    "                    \n",
    "                    # This probably won't happen though at the node level\n",
    "                    # If there are no incoming edges:\n",
    "                    if graph.in_degree(node) == 0:\n",
    "                        \n",
    "                        # Add to all_seeds list\n",
    "                        all_seeds.append(node)\n",
    "\n",
    "                        successors_list = find_successors(seed = node, SCC = list(node), graph = graph,\\\n",
    "                                                          distance_transformation = distance_transformation, \\\n",
    "                                                          all_node_weights = all_node_weights)\n",
    "\n",
    "                        # Make a dictionary for this seed group\n",
    "                        seedgroup = {\"seedGroupID\": seedgroup_ID,\\\n",
    "                                     \"seedCompoundIDs\": list(node),\\\n",
    "                                     \"nonSeedSuccessors\": successors_list}\n",
    "\n",
    "                        # Add it to the list of seed groups\n",
    "                        all_seedgroups.append(seedgroup)\n",
    "                        seedgroup_ID += 1\n",
    "                        \n",
    "            #------------IF SCC SIZE == 1------------\n",
    "            \n",
    "            # If the SCC only has one node\n",
    "            elif len(SCC_lst) == 1:\n",
    "                \n",
    "                # Check that there are no incoming edges\n",
    "                if graph.in_degree(SCC_lst[0]) == 0:\n",
    "                    \n",
    "                    # Add to all_seeds list\n",
    "                    all_seeds.append(SCC_lst[0])\n",
    "\n",
    "                    successors_list = find_successors(seed = SCC_lst[0], SCC = SCC_lst, graph = graph,\\\n",
    "                                                      distance_transformation = distance_transformation,\\\n",
    "                                                      all_node_weights = all_node_weights)\n",
    "\n",
    "                    # Make a dictionary for this seed group\n",
    "                    seedgroup = {\"seedGroupID\": seedgroup_ID,\\\n",
    "                                 \"seedCompoundIDs\": SCC_lst,\\\n",
    "                                 \"nonSeedSuccessors\": successors_list}\n",
    "                    \n",
    "                    # Add it to the list of seed groups\n",
    "                    all_seedgroups.append(seedgroup)\n",
    "                    seedgroup_ID += 1\n",
    "                    \n",
    "                   \n",
    "            #------------IF 1 < SCC SIZE < MAX SIZE------------\n",
    "            \n",
    "            # If the SCC has between 2 and the maximum threshold of nodes\n",
    "            else:\n",
    "                \n",
    "                # Check that SCC is self-contained (i.e., all in-edges are in the SCC)\n",
    "                SCC_inedges = [edge[0] for node in SCC_lst \\\n",
    "                              for edge in graph.in_edges(node) \\\n",
    "                              if edge[0] not in SCC_lst]\n",
    "                \n",
    "                # Check that SCC_inedges is empty (== the SCC is self-contained)\n",
    "                if not SCC_inedges:\n",
    "                    \n",
    "                    # Initialize SCC_successors (will be updated after going through seeds)\n",
    "                    SCC_successors = []\n",
    "        \n",
    "                    for seed in SCC_lst:\n",
    "                \n",
    "                        # Add to seeds dictionary\n",
    "                        all_seeds.append(seed)\n",
    "                        \n",
    "                        successors_list = find_successors(seed = seed, SCC = SCC_lst, graph = graph,\\\n",
    "                                                          distance_transformation = distance_transformation, \\\n",
    "                                                          all_node_weights = all_node_weights)\n",
    "\n",
    "                        for successor_dict in successors_list:\n",
    "                            \n",
    "                            successor_location = next((i for i, j in enumerate(SCC_successors)\\\n",
    "                                                       if successor_dict[\"compoundID\"] == j[\"compoundID\"]), None)\n",
    "                            \n",
    "                            # If that nonseed successor is NOT already in SCC_successors:\n",
    "                            if successor_location is None:\n",
    "                                SCC_successors.append(successor_dict)\n",
    "                             \n",
    "                            # Else if it's in SCC_successors, update scaled_distance if it's greater this time \n",
    "                            # (i.e., if the original distance was smaller)\n",
    "                            else:\n",
    "                                previous_distance = SCC_successors[successor_location][\"distance\"]\n",
    "                                if successor_dict[\"distance\"] > previous_distance:\n",
    "                                    SCC_successors[successor_location][\"distance\"] = successor_dict[\"distance\"]\n",
    "                                \n",
    "                    # Make a dictionary for this seed group\n",
    "                    seedgroup = {\"seedGroupID\": seedgroup_ID,\\\n",
    "                                 \"seedCompoundIDs\": SCC_lst,\\\n",
    "                                 \"nonSeedSuccessors\": SCC_successors}\n",
    "                    \n",
    "                    # Add it to the list of seed groups\n",
    "                    all_seedgroups.append(seedgroup)\n",
    "                    seedgroup_ID += 1\n",
    "                    \n",
    "                    \n",
    "        #------------FIND ALL_NONSEEDS AND THEIR \"PARENT\" SEED GROUPS------------\n",
    "        \n",
    "        # Initialize all_nonseeds\n",
    "        # Will become a list of dictionaries in the form:\n",
    "        #    [\n",
    "        #        {\n",
    "        #        \"compoundID\": \n",
    "        #        \"weight\":\n",
    "        #        \"seedGroupPredecessors\": \n",
    "        #            [\n",
    "        #                {\n",
    "        #                \"seedGroupID\" :\n",
    "        #                \"distance\" :\n",
    "        #                }\n",
    "        #            ]\n",
    "        #        }\n",
    "        #    ]\n",
    "        all_nonseeds = []\n",
    "        \n",
    "        # Grab nonseed IDs\n",
    "        all_nonseed_IDs = list(set(all_nodes) - set(all_seeds))\n",
    "        number_of_nonseeds = len(all_nonseed_IDs)\n",
    "        \n",
    "        # Will hold any times there's only one seed group that has that nonseed as a successor\n",
    "        #      {\"seedGroupID\": [\"successorCompoundID\", ....]}\n",
    "        uniqueimpacts_of_seedgroup_predecessors = {}\n",
    "        \n",
    "        #------------\n",
    "        # Iterate over all nonseed IDs\n",
    "        for nonseed_ID in all_nonseed_IDs:\n",
    "            \n",
    "            # Find weight of that specific nonseed\n",
    "            nonseed_weight = all_node_weights[nonseed_ID]\n",
    "            \n",
    "            #------------\n",
    "            # Find seed groups predecessors (i.e., which seed groups is this nonseed a successor of?)\n",
    "            seedgroup_predecessors = []\n",
    "            \n",
    "            for seedgroup in all_seedgroups:\n",
    "                \n",
    "                # Check if this nonseed is one of its successors\n",
    "                successor_location = next((i for i, j in enumerate(seedgroup[\"nonSeedSuccessors\"])\\\n",
    "                                           if j[\"compoundID\"] == nonseed_ID), None)\n",
    "                # If it is\n",
    "                if successor_location is not None:\n",
    "                    seedgroup_predecessor = {\"seedGroupID\" : seedgroup[\"seedGroupID\"],\\\n",
    "                                             \"seedCompoundIDs\" : seedgroup[\"seedCompoundIDs\"],\\\n",
    "                                             \"distance\" : seedgroup[\"nonSeedSuccessors\"][successor_location][\"distance\"]}\n",
    "                \n",
    "                    seedgroup_predecessors.append(seedgroup_predecessor)\n",
    "                \n",
    "            #------------\n",
    "            # If there's only one, grab as unique contribution\n",
    "            if len(seedgroup_predecessors)==1:\n",
    "                \n",
    "                seedgroup_predecessor_ID = seedgroup_predecessors[0][\"seedGroupID\"]\n",
    "                \n",
    "                # Check if it's already in that dictionary\n",
    "                if seedgroup_predecessor_ID not in uniqueimpacts_of_seedgroup_predecessors.keys():\n",
    "                    uniqueimpacts_of_seedgroup_predecessors[seedgroup_predecessor_ID] = [nonseed_ID]\n",
    "                    \n",
    "                else:\n",
    "                    uniqueimpacts_of_seedgroup_predecessors[seedgroup_predecessor_ID].append(nonseed_ID)\n",
    "            \n",
    "            #------------\n",
    "            # Check for errors\n",
    "            elif len(seedgroup_predecessors)==0:\n",
    "                print(\"Error: no seed group predecessors found for nonseed: \", str(nonseed_ID))\n",
    "                    \n",
    "            #------------\n",
    "            # Make a dictionary for this nonseed\n",
    "            nonseed = {\"compoundID\": nonseed_ID,\\\n",
    "                       \"weight\": nonseed_weight,\\\n",
    "                       \"seedGroupPredecessors\": seedgroup_predecessors}\n",
    "                       \n",
    "            # Add it to the list of nonseeds\n",
    "            all_nonseeds.append(nonseed)\n",
    "                   \n",
    "        #------------FIND IMPACT SCORE FOR EACH \"PARENT\" SEED GROUP------------\n",
    "        # Seems confusing, but the easiest way was to compute the non-seeds and their parents first\n",
    "        \n",
    "        for seedgroup in all_seedgroups:\n",
    "            \n",
    "            seedgroup_ID = seedgroup[\"seedGroupID\"]\n",
    "            unique_impact = 0.0\n",
    "            total_impact = 0.0\n",
    "                       \n",
    "            # Calculate the unique impact of this seed group\n",
    "            if seedgroup_ID in uniqueimpacts_of_seedgroup_predecessors.keys():\n",
    "                unique_nonseeds = uniqueimpacts_of_seedgroup_predecessors[seedgroup_ID]\n",
    "                \n",
    "                for nonseed_ID in unique_nonseeds:\n",
    "                    nonseed_location = next((i for i, j in enumerate(seedgroup[\"nonSeedSuccessors\"])\\\n",
    "                                           if j[\"compoundID\"] == nonseed_ID), None)\n",
    "                    nonseed_distance = seedgroup[\"nonSeedSuccessors\"][nonseed_location][\"distance\"]\n",
    "                    nonseed_weight = seedgroup[\"nonSeedSuccessors\"][nonseed_location][\"weight\"]\n",
    "                    nonseed_impact = (nonseed_distance * nonseed_weight) / number_of_nonseeds\n",
    "                    unique_impact += nonseed_impact\n",
    "                       \n",
    "            else:\n",
    "                unique_nonseeds = None\n",
    "                    \n",
    "            # Calculate the total impact of this seed group\n",
    "            for successor_dict in seedgroup[\"nonSeedSuccessors\"]:\n",
    "                nonseed_impact = (successor_dict[\"distance\"] * successor_dict[\"weight\"]) / number_of_nonseeds\n",
    "                total_impact += nonseed_impact\n",
    "                       \n",
    "            seedgroup[\"networkImpact\"] = {}\n",
    "            seedgroup[\"networkImpact\"][\"total\"] = {\"score\": total_impact}\n",
    "            seedgroup[\"networkImpact\"][\"unique\"] = {\"score\": unique_impact,\\\n",
    "                                                    \"nonSeedIDs\": unique_nonseeds}\n",
    "            \n",
    "                       \n",
    "        #------------RETURN EVERYTHING------------\n",
    "        \n",
    "        #print(len(nodes), len(non_seeds), len(seeds))\n",
    "        \n",
    "        # all_seeds = List of compound IDs\n",
    "        # all_seedgroups = List of dictionaries, one for each seed group\n",
    "        # all_nonseeds = List of dictionaries, one for each nonseed\n",
    "        # all_nodes = List of compound IDs\n",
    "        return([all_seeds, all_seedgroups, all_nonseeds, all_nodes])\n",
    "\n",
    "      \n",
    "    #---------------------------------------------------------\n",
    "\n",
    "    def update_with_seeds(org_dict, all_seeds, all_seedgroups, all_nonseeds, all_nodes):\n",
    "\n",
    "        for cat_dict in org_dict[\"organism\"][\"categories\"]:\n",
    "            for path_dict in cat_dict[\"pathways\"]:\n",
    "                for reaction_dict in path_dict[\"reactions\"]:\n",
    "                    for compound_dict in reaction_dict[\"compounds\"]:\n",
    "                        \n",
    "                        compound_ID = compound_dict[\"compoundID\"]\n",
    "                        all_nonseed_IDs = [n[\"compoundID\"] for n in all_nonseeds]\n",
    "                        \n",
    "                        if compound_ID in all_seeds:\n",
    "                            compound_dict[\"isSeed\"] = True\n",
    "\n",
    "                            # Find which seedgroup it's in\n",
    "                            seed_group_location = next((i for i, j in enumerate(all_seedgroups)\\\n",
    "                                                        if compound_ID in j[\"seedCompoundIDs\"]), None)\n",
    "                            \n",
    "                            compound_dict[\"seedGroupID\"] = all_seedgroups[seed_group_location][\"seedGroupID\"]\n",
    "\n",
    "                        elif compound_ID in all_nonseed_IDs:\n",
    "                            compound_dict[\"isSeed\"] = False\n",
    "                            compound_dict[\"seedGroupID\"] = None\n",
    "\n",
    "                        else:\n",
    "                            print(\"Error, not in seeds or nonseeds\", compound_ID)\n",
    "\n",
    "        org_dict[\"organism\"][\"allNodes\"] = all_nodes\n",
    "        org_dict[\"organism\"][\"allSeeds\"] = all_seeds\n",
    "        org_dict[\"organism\"][\"allSeedGroups\"] = all_seedgroups\n",
    "        org_dict[\"organism\"][\"allNonSeeds\"] = all_nonseeds\n",
    "\n",
    "        return org_dict\n",
    "\n",
    "    #---------------------------------------------------------\n",
    "    # Extract edge-notation file\n",
    "    edges = extract_TSV(org_dict = org_dict)\n",
    "    \n",
    "    # Calculate the seed sets\n",
    "    all_seeds, all_seedgroups, all_nonseeds, all_nodes = calculate_seeds(edges)\n",
    "    \n",
    "    # Update the data structure with seed information\n",
    "    org_dict = update_with_seeds(org_dict = org_dict, \\\n",
    "                                 all_seeds = all_seeds, all_seedgroups = all_seedgroups,\\\n",
    "                                 all_nonseeds= all_nonseeds, all_nodes = all_nodes)\n",
    "    \n",
    "    # Print progress\n",
    "    print(\"Step 4 complete.\")\n",
    "    \n",
    "    # Return\n",
    "    return org_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step5'></a>\n",
    "## <span style=\"margin:auto;display:table\">Step 5: Send to Files</span>\n",
    "\n",
    "\n",
    "#### Required input:\n",
    "> Output from Step 4 <br>\n",
    "> ```genome_input``` <br>\n",
    "> ```all_compound_names```\n",
    "\n",
    "#### Overview of process:\n",
    "\n",
    "The final data structure from Step 4 is written to several files:\n",
    ">- A JSON file containing the entire structure. <br> <br>\n",
    ">- A CSV file showing the hierarchy of category, pathway, reaction, and compound, as well as whether each compound is a <span style=\"color:red;\">seed</span> or a <span style=\"color:blue;\">non-seed</span>. <br> <br>\n",
    ">- A CSV file showing the relationships between <span style=\"color:red;\">seed\"parents\"</span> and their <span style=\"color:blue;\">non-seed \"children\"</span>.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Send full dictionary structure to JSON file\n",
    "2) Create pandas DataFrame, where each row corresponds to a compound.\n",
    "   For that compound, show which reaction, pathway, and category it's in,\n",
    "   as well as whether or not it is a seed/non-seed.\n",
    "3) Send DataFrame from Step 2 to CSV file.\n",
    "4) Create pandas DataFrame, where each row corresponds to a unique\n",
    "   child non-seed and parent seed relationship. \n",
    "5) Send DataFrame from Step 4 to CSV file.\n",
    "\n",
    "Does not return anything.\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def step5(org_dict, genome_input, all_compound_names):\n",
    "    \n",
    "    #------------SEND ENTIRE DICTIONARY TO JSON FILE------------\n",
    "    with open(genome_input[\"destination_filepath_json\"], \"w\") as f:\n",
    "        json.dump(org_dict, f)\n",
    "      \n",
    "    #------------SEND ENTIRE DICTIONARY TO CSV FILE------------\n",
    "    org_df = pd.DataFrame()\n",
    "    org_nonseeds_df = pd.DataFrame()\n",
    "\n",
    "    organismID = org_dict[\"organism\"][\"organismID\"]\n",
    "    organismName = org_dict[\"organism\"][\"organismName\"]\n",
    "    organismSource = org_dict[\"organism\"][\"organismSource\"]\n",
    "\n",
    "    #------------\n",
    "    #Category - Pathway - Reaction - Compound\n",
    "    for cat_dict in org_dict[\"organism\"][\"categories\"]:\n",
    "\n",
    "        categoryName = cat_dict[\"categoryName\"]\n",
    "\n",
    "        for path_dict in cat_dict[\"pathways\"]:\n",
    "\n",
    "            pathwayID = path_dict[\"pathwayID\"]\n",
    "            pathwayName = path_dict[\"pathwayName\"]\n",
    "\n",
    "            for reaction_dict in path_dict[\"reactions\"]:\n",
    "\n",
    "                reactionID = reaction_dict[\"reactionID\"]\n",
    "                reactionType = reaction_dict[\"reactionType\"]\n",
    "                KOnumbers = ','.join(reaction_dict[\"KOnumbers\"])\n",
    "                edges = ','.join(reaction_dict[\"edges\"])\n",
    "\n",
    "                for compound_dict in reaction_dict[\"compounds\"]:\n",
    "                    compoundID = compound_dict[\"compoundID\"]\n",
    "                    compoundName = compound_dict[\"compoundName\"]\n",
    "                    compoundType = compound_dict[\"compoundType\"]\n",
    "                    isSeed = compound_dict[\"isSeed\"]\n",
    "                    seedGroupID = compound_dict[\"seedGroupID\"]\n",
    "\n",
    "                    org_df_row = {'Organism ID': organismID,\\\n",
    "                                     'Organism Name': organismName,\\\n",
    "                                     'Organism Source': organismSource,\\\n",
    "                                     'Category Name': categoryName,\\\n",
    "                                     'Pathway ID' : pathwayID,\\\n",
    "                                     'Pathway Name' : pathwayName,\\\n",
    "                                     'Reaction ID' : reactionID,\\\n",
    "                                     'Reaction Type' : reactionType,\\\n",
    "                                     'Reaction KO Numbers' : KOnumbers,\\\n",
    "                                     'Reaction Edge-Notation' : edges,\\\n",
    "                                     'Compound ID' : compoundID,\\\n",
    "                                     'Compound Name' : compoundName,\\\n",
    "                                     'Compound Type' : compoundType,\\\n",
    "                                     'Is Compound Seed?' : isSeed,\\\n",
    "                                     'Compound Seed Group' : seedGroupID}\n",
    "\n",
    "                    org_df = org_df.append(org_df_row, ignore_index = True)\n",
    "\n",
    "    org_df.to_csv(genome_input[\"destination_filepath_csv\"], index=False,\\\n",
    "                  columns = ['Organism Name','Organism Source','Organism ID',\\\n",
    "                             'Category Name','Pathway Name','Pathway ID',\\\n",
    "                             'Reaction ID','Reaction Type','Reaction KO Numbers',\\\n",
    "                             'Reaction Edge-Notation','Compound ID','Compound Name',\\\n",
    "                             'Compound Type','Is Compound Seed?','Compound Seed Group'])\n",
    "           \n",
    "    #------------\n",
    "    #Child Nonseed and Parent Seeds Relationships   \n",
    "    for nonseed_dict in org_dict[\"organism\"][\"allNonSeeds\"]:\n",
    "        nonseed_compound_ID = nonseed_dict[\"compoundID\"]\n",
    "        nonseed_weight = nonseed_dict[\"weight\"]\n",
    "        nonseed_compound_name = all_compound_names[nonseed_compound_ID]\n",
    "        \n",
    "        for seed_group_predecessor in nonseed_dict[\"seedGroupPredecessors\"]:\n",
    "            seed_group_ID = seed_group_predecessor[\"seedGroupID\"]\n",
    "            seed_group_distance = seed_group_predecessor[\"distance\"]\n",
    "            \n",
    "            for seed_predecessor in seed_group_predecessor[\"seedCompoundIDs\"]:\n",
    "                seed_predecessor_compound_name = all_compound_names[seed_predecessor]\n",
    "                \n",
    "                org_nonseeds_df_row = {'Organism ID': organismID,\\\n",
    "                                       'Organism Name': organismName,\\\n",
    "                                       'Organism Source': organismSource,\\\n",
    "                                       \"Nonseed Compound ID\": nonseed_compound_ID,\\\n",
    "                                       \"Nonseed Compound Name\": nonseed_compound_name,\\\n",
    "                                       \"Nonseed Weight\": nonseed_weight,\\\n",
    "                                       \"Seed Group Predecessor - ID\": seed_group_ID,\\\n",
    "                                       \"Seed Group Predecessor - Distance\": seed_group_distance,\\\n",
    "                                       \"Seed Group Predecessor - Seed Compound ID\": seed_predecessor,\\\n",
    "                                       \"Seed Group Predecessor - Seed Compound Name\": seed_predecessor_compound_name}\n",
    "            \n",
    "                org_nonseeds_df = org_nonseeds_df.append(org_nonseeds_df_row, ignore_index = True)\n",
    "        \n",
    "    org_nonseeds_df.to_csv(genome_input[\"destination_filepath_csv_nonseeds\"], index=False,\\\n",
    "                  columns = ['Organism Name',\\\n",
    "                             'Organism Source',\\\n",
    "                             'Organism ID',\\\n",
    "                             \"Nonseed Compound ID\",\\\n",
    "                             \"Nonseed Weight\",\\\n",
    "                             \"Seed Group Predecessor - ID\",\\\n",
    "                             \"Seed Group Predecessor - Distance\",\\\n",
    "                             \"Seed Group Predecessor - Seed Compound ID\"])\n",
    "\n",
    "    # Print progress\n",
    "    print(\"Step 5 complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step6'></a>\n",
    "## <span style=\"margin:auto;display:table;\">Step 6: Visualize Network</span>\n",
    "\n",
    "#### Required input:\n",
    "> ```genome_inputs``` <br>\n",
    "\n",
    "#### Overview of process:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Extract tab-delimited edges (i.e., reactions) from network.\n",
    "2) Define colors for each node (i.e., compound), so that seeds are red and non-seeds are blue.\n",
    "3) Draw network using networkx.\n",
    "4) Save network to PDF file.\n",
    "\n",
    "Does not return anything.\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def step6(org_dict, genome_input):\n",
    "    \n",
    "    # Extract tab-delimited edges (i.e., reactions) from network\n",
    "    data = extract_TSV(org_dict = org_dict)\n",
    "        \n",
    "    # Grab all compounds and seeds\n",
    "    allnodes = set(org_dict[\"organism\"][\"allNodes\"])\n",
    "    seeds = set(org_dict[\"organism\"][\"allSeeds\"])\n",
    "    \n",
    "    # Initialize map that will determine colors for each compound\n",
    "    color_map = {}\n",
    "\n",
    "    # Color all seeds red\n",
    "    for s in seeds:\n",
    "        color_map[s] = 'red'\n",
    "    # Color all non-seeds blue\n",
    "    for s in [n for n in allnodes if n not in seeds]:\n",
    "        color_map[s] = 'blue'\n",
    "                \n",
    "    # Initialize networkx graph using extracted TSV\n",
    "    G = nx.parse_edgelist(data, delimiter='\\t', create_using = nx.DiGraph())\n",
    "    \n",
    "    # Find a color for each compound (i.e., node) based on the color_map\n",
    "    values = [color_map.get(node) for node in G.nodes()]\n",
    "    \n",
    "    # Intialize figure\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # Draw network\n",
    "    nx.draw_kamada_kawai(G, node_size = 10, arrowsize = 0.4, alpha = 0.2, node_color=values)\n",
    "    \n",
    "    # Save figure to file\n",
    "    plt.savefig(genome_input[\"destination_filepath_pdf\"], bbox_inches='tight')\n",
    "    \n",
    "    # Close figure so that it won't pop up after completion\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Print progress\n",
    "    print(\"Step 6 complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Step7'></a>\n",
    "## <span style=\"margin:auto;display:table;\">Step 7: Summary Statistics and Network Comparisons</span>\n",
    "\n",
    "#### Required input:\n",
    "> ```all_inputs``` <br>\n",
    "\n",
    "#### Overview of process:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Find all genomes that have completed metabolic networks\n",
    "2) For each completed network, extract summary statistics \n",
    "   (i.e., # reactions, # compounds, # seeds, % compounds that are seeds),\n",
    "   as well as sets of reactions, compounds, and seeds to be used in pair-wise comparisons\n",
    "3) Print summary statistics to CSV file.\n",
    "3) Get all unique pairs of completed networks\n",
    "4) For each pair of networks, find overlaps, differences, and an overall Jaccard similarity score\n",
    "   in terms of their reactions, compounds, and seeds.\n",
    "5) Print pair-wise comparisons to CSV file.\n",
    "\n",
    "Does not return anything.\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def step7(all_inputs):\n",
    "    \n",
    "    #-------------------------    \n",
    "    # Find all genomes that have completed metabolic networks\n",
    "    complete_inputs = [i for i in all_inputs if i[\"partB_network_complete\"] == True]\n",
    "    \n",
    "    \n",
    "    # Make Summary Statistics Directory\n",
    "    try:\n",
    "        os.makedirs(\"./Summary Statistics\")\n",
    "    # Don't make anything if the directory already exists\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                    \n",
    "    # Create empty pandas DataFrame to hold summary statistics \n",
    "    # (i.e.,  # reactions, # seeds, # compounds, % seeds)\n",
    "    summary_stats_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over completed networks\n",
    "    for complete_input in complete_inputs:\n",
    "        \n",
    "        # Open network file\n",
    "        with open(complete_input[\"destination_filepath_json\"], \"r\") as f:\n",
    "            \n",
    "            # Load network\n",
    "            network = json.load(f)\n",
    "\n",
    "            # Extract all reactions, seeds, and compounds\n",
    "            # (to be used in pair-wise comparisons)\n",
    "            reaction_set = set(extract_TSV(org_dict = network))\n",
    "            seed_set = set(network[\"organism\"][\"allSeeds\"])\n",
    "            compound_set = set(network[\"organism\"][\"allNodes\"])\n",
    "\n",
    "            # Add sets to complete_input dictionary\n",
    "            complete_input[\"reaction_set\"] = reaction_set\n",
    "            complete_input[\"seed_set\"] = seed_set\n",
    "            complete_input[\"compound_set\"] = compound_set\n",
    "            \n",
    "            # Extract summary statistics for this network\n",
    "            summary_stats_row = {\"Organism Name\" : complete_input[\"organism_name\"],\\\n",
    "                                 \"Organism ID\" : complete_input[\"organism_ID\"],\\\n",
    "                                 \"Organism Source\" : complete_input[\"database\"],\\\n",
    "                                 \"# of Reactions\" : len(reaction_set),\\\n",
    "                                 \"# of Seeds\" : len(seed_set),\\\n",
    "                                 \"# of Compounds\" : len(compound_set),\\\n",
    "                                 \"% of Compounds that are Seeds\" : len(seed_set) / len(compound_set)}\n",
    "            \n",
    "            # Append to DataFrame\n",
    "            summary_stats_df = summary_stats_df.append(summary_stats_row, ignore_index = True)\n",
    "        \n",
    "    # Print summary statistics DataFrame to file\n",
    "    summary_stats_df.to_csv(\"./Summary Statistics/Summary Statistics.csv\", index = False,\\\n",
    "                           columns = [\"Organism Name\", \"Organism ID\" ,\"Organism Source\",\\\n",
    "                                      \"# of Reactions\", \"# of Seeds\", \"# of Compounds\", \\\n",
    "                                      \"% of Compounds that are Seeds\"])\n",
    "    \n",
    "    #-------------------------    \n",
    "    # Create empty pandas DataFrame to hold pair-wise comparisons\n",
    "    comparison_df = pd.DataFrame()\n",
    "    \n",
    "    # Get all unique pairs of completed networks\n",
    "    combos = itertools.combinations(complete_inputs, 2)\n",
    "    \n",
    "    # Iterate over pairs\n",
    "    for combo in combos:\n",
    "        \n",
    "        # Find differences, overlap, and Jaccard similarity in terms of reactions\n",
    "        reactions_only_in_A = len(combo[0][\"reaction_set\"] - combo[1][\"reaction_set\"])\n",
    "        reactions_only_in_B = len(combo[1][\"reaction_set\"] - combo[0][\"reaction_set\"])\n",
    "        reactions_in_both = len(combo[0][\"reaction_set\"] & combo[1][\"reaction_set\"])\n",
    "        reactions_in_either = len(combo[0][\"reaction_set\"] | combo[1][\"reaction_set\"])\n",
    "        reactions_jaccard_similarity = reactions_in_both / reactions_in_either\n",
    "        \n",
    "        # Find differences, overlap, and Jaccard similarity in terms of compounds\n",
    "        compounds_only_in_A = len(combo[0][\"compound_set\"] - combo[1][\"compound_set\"])\n",
    "        compounds_only_in_B = len(combo[1][\"compound_set\"] - combo[0][\"compound_set\"])\n",
    "        compounds_in_both = len(combo[0][\"compound_set\"] & combo[1][\"compound_set\"])\n",
    "        compounds_in_either = len(combo[0][\"compound_set\"] | combo[1][\"compound_set\"])\n",
    "        compounds_jaccard_similarity = compounds_in_both / compounds_in_either\n",
    "        \n",
    "        # Find differences, overlap, and Jaccard similarity in terms of seeds\n",
    "        seeds_only_in_A = len(combo[0][\"seed_set\"] - combo[1][\"seed_set\"])\n",
    "        seeds_only_in_B = len(combo[1][\"seed_set\"] - combo[0][\"seed_set\"])\n",
    "        seeds_in_both = len(combo[0][\"seed_set\"] & combo[1][\"seed_set\"])\n",
    "        seeds_in_either = len(combo[0][\"seed_set\"] | combo[1][\"seed_set\"])\n",
    "        seeds_jaccard_similarity = seeds_in_both / seeds_in_either\n",
    "        \n",
    "        # Compile DataFrame row\n",
    "        comparison_df_row1 = {\"Network A - Organism Name\" : combo[0][\"organism_name\"],\\\n",
    "                             \"Network A - Organism ID\" : combo[0][\"organism_ID\"],\\\n",
    "                             \"Network A - Organism Source\" : combo[0][\"database\"],\\\n",
    "                             \"Network B - Organism Name\" : combo[1][\"organism_name\"],\\\n",
    "                             \"Network B - Organism ID\" : combo[1][\"organism_ID\"],\\\n",
    "                             \"Network B - Organism Source\" : combo[1][\"database\"],\\\n",
    "                             \"Reactions only in Network A\": reactions_only_in_A ,\\\n",
    "                             \"Reactions only in Network B\": reactions_only_in_B,\\\n",
    "                             \"Reactions in both\": reactions_in_both,\\\n",
    "                             \"Jaccard similarity of reactions\": reactions_jaccard_similarity,\\\n",
    "                             \"Compounds only in Network A\": compounds_only_in_A ,\\\n",
    "                             \"Compounds only in Network B\": compounds_only_in_B,\\\n",
    "                             \"Compounds in both\": compounds_in_both,\\\n",
    "                             \"Jaccard similarity of compounds\": compounds_jaccard_similarity,\\\n",
    "                             \"Seeds only in Network A\": seeds_only_in_A ,\\\n",
    "                             \"Seeds only in Network B\": seeds_only_in_B,\\\n",
    "                             \"Seeds in both\": seeds_in_both,\\\n",
    "                             \"Jaccard similarity of seeds\": seeds_jaccard_similarity}\n",
    "        \n",
    "        # Compile DataFrame row (networks flipped)\n",
    "        comparison_df_row2 = {\"Network A - Organism Name\" : combo[1][\"organism_name\"],\\\n",
    "                             \"Network A - Organism ID\" : combo[1][\"organism_ID\"],\\\n",
    "                             \"Network A - Organism Source\" : combo[1][\"database\"],\\\n",
    "                             \"Network B - Organism Name\" : combo[0][\"organism_name\"],\\\n",
    "                             \"Network B - Organism ID\" : combo[0][\"organism_ID\"],\\\n",
    "                             \"Network B - Organism Source\" : combo[0][\"database\"],\\\n",
    "                             \"Reactions only in Network A\": reactions_only_in_B ,\\\n",
    "                             \"Reactions only in Network B\": reactions_only_in_A,\\\n",
    "                             \"Reactions in both\": reactions_in_both,\\\n",
    "                             \"Jaccard similarity of reactions\": reactions_jaccard_similarity,\\\n",
    "                             \"Compounds only in Network A\": compounds_only_in_B,\\\n",
    "                             \"Compounds only in Network B\": compounds_only_in_A,\\\n",
    "                             \"Compounds in both\": compounds_in_both,\\\n",
    "                             \"Jaccard similarity of compounds\": compounds_jaccard_similarity,\\\n",
    "                             \"Seeds only in Network A\": seeds_only_in_B ,\\\n",
    "                             \"Seeds only in Network B\": seeds_only_in_A,\\\n",
    "                             \"Seeds in both\": seeds_in_both,\\\n",
    "                             \"Jaccard similarity of seeds\": seeds_jaccard_similarity}\n",
    "        \n",
    "        # Append to DataFrame\n",
    "        comparison_df = comparison_df.append(comparison_df_row1, ignore_index = True)\n",
    "        comparison_df = comparison_df.append(comparison_df_row2, ignore_index = True)\n",
    "        \n",
    "    # Print pair-wise comparisons DataFrame to file\n",
    "    comparison_df.to_csv(\"./Summary Statistics/Comparisons.csv\", index = False,\\\n",
    "                        columns = [\"Network A - Organism Name\",\\\n",
    "                                   \"Network A - Organism ID\",\\\n",
    "                                   \"Network A - Organism Source\",\\\n",
    "                                   \"Network B - Organism Name\",\\\n",
    "                                   \"Network B - Organism ID\",\\\n",
    "                                   \"Network B - Organism Source\",\\\n",
    "                                   \"Reactions only in Network A\",\\\n",
    "                                   \"Reactions only in Network B\",\\\n",
    "                                   \"Reactions in both\",\\\n",
    "                                   \"Jaccard similarity of reactions\",\\\n",
    "                                   \"Compounds only in Network A\",\\\n",
    "                                   \"Compounds only in Network B\",\\\n",
    "                                   \"Compounds in both\",\\\n",
    "                                   \"Jaccard similarity of compounds\",\\\n",
    "                                   \"Seeds only in Network A\",\\\n",
    "                                   \"Seeds only in Network B\",\\\n",
    "                                   \"Seeds in both\",\\\n",
    "                                   \"Jaccard similarity of seeds\"])\n",
    "    #-------------------------\n",
    "    # Make heatmaps\n",
    "    comparison_df[\"Network A Label\"] = comparison_df[\"Network A - Organism Name\"] + \" - \" + comparison_df[\"Network A - Organism Source\"] + \" - \" + comparison_df[\"Network A - Organism ID\"]\n",
    "    comparison_df[\"Network B Label\"] = comparison_df[\"Network B - Organism Name\"] + \" - \" + comparison_df[\"Network B - Organism Source\"] + \" - \" + comparison_df[\"Network B - Organism ID\"]\n",
    "\n",
    "    heatmap_df_seeds = comparison_df[[\"Network A Label\", \"Network B Label\", \"Jaccard similarity of seeds\"]]\n",
    "    heatmap_df_reactions = comparison_df[[\"Network A Label\", \"Network B Label\", \"Jaccard similarity of reactions\"]]\n",
    "    heatmap_df_compounds = comparison_df[[\"Network A Label\", \"Network B Label\", \"Jaccard similarity of compounds\"]]\n",
    "\n",
    "    # Pivot DataFrames\n",
    "    heatmap_pivotdf_seeds = heatmap_df_seeds.pivot_table(index='Network A Label', columns='Network B Label', \\\n",
    "                                                         values=\"Jaccard similarity of seeds\")\n",
    "    heatmap_pivotdf_reactions = heatmap_df_reactions.pivot_table(index='Network A Label', columns='Network B Label', \\\n",
    "                                                         values=\"Jaccard similarity of reactions\")\n",
    "    heatmap_pivotdf_compounds = heatmap_df_compounds.pivot_table(index='Network A Label', columns='Network B Label', \\\n",
    "                                                         values=\"Jaccard similarity of compounds\")\n",
    "\n",
    "    df_title_dict = {\"\\nJaccard Similarity of Seeds\\n\" :heatmap_pivotdf_seeds,\\\n",
    "                     \"\\nJaccard Similarity of Reactions\\n\" :heatmap_pivotdf_reactions,\\\n",
    "                     \"\\nJaccard Similarity of Compounds\\n\" :heatmap_pivotdf_compounds}\n",
    "\n",
    "    for title,df in df_title_dict.items():\n",
    "\n",
    "        filepath = \"./Summary Statistics/\" + title.strip()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(40,40))\n",
    "        ax.set_title(title, fontsize=30)\n",
    "        ax.set_xlabel(\"\", fontsize=30)\n",
    "        ax.set_ylabel(\"\", fontsize=30)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "        sns.heatmap(df, ax = ax, cmap='coolwarm')\n",
    "        plt.tight_layout()\n",
    "        # Save figure to file\n",
    "        plt.savefig(filepath, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    #-------------------------\n",
    "    # Print progress\n",
    "    print(\"Step 7 complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<a id='Full_Pipeline_PartB'></a>\n",
    "## <span style=\"margin:auto;display:table\">Full Pipeline for Part B</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "1) Use the KEGG API to find all compound names\n",
    "2) Iterate over genome inputs; if part B isn't already complete, run steps 3-6.\n",
    "3) Run step 7 to compute summary statistics for each network \n",
    "   as well as pair-wise comparisons between networks.\n",
    "\n",
    "Does not return anything.\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def full_pipeline_partB(all_inputs,\\\n",
    "                        max_component_size = 5, \\\n",
    "                        distance_transformation = \"1/x\", \\\n",
    "                        weight_measure = \"Degree Centrality\"):\n",
    "    \n",
    "    #------------GET DICTIONARY OF COMPOUND NAMES {ID:Name}------------\n",
    "\n",
    "    all_compound_names = {}\n",
    "    compounds_str = REST.kegg_list(\"compound\").read()\n",
    "    glycans_str = REST.kegg_list(\"glycan\").read()\n",
    "\n",
    "    for line in compounds_str.rstrip().split(\"\\n\"):\n",
    "        compound_ID, compound_name = line.split(\"\\t\")\n",
    "        all_compound_names[compound_ID[4:]] = compound_name\n",
    "\n",
    "    for line in glycans_str.rstrip().split(\"\\n\"):\n",
    "        glycan_ID, glycan_name = line.split(\"\\t\")\n",
    "        all_compound_names[glycan_ID[3:]] = glycan_name\n",
    "\n",
    "    #-------------------------   \n",
    "    # Iterate over inputs (one per network)\n",
    "    for genome_input in all_inputs:\n",
    "        \n",
    "        # Print which genome is currently in progress\n",
    "        print(genome_input[\"organism_name\"], genome_input[\"database\"], genome_input[\"organism_ID\"])\n",
    "        \n",
    "        # Check that the network isn't already complete\n",
    "        if genome_input[\"partB_network_complete\"] == True:\n",
    "            print(\"Part B already complete for this genome.\")\n",
    "        \n",
    "        else:\n",
    "            step3_output = step3(genome_input = genome_input, \\\n",
    "                                 all_compound_names = all_compound_names)\n",
    "            step4_output = step4(org_dict = step3_output)\n",
    "            step5(org_dict = step4_output, genome_input = genome_input,\\\n",
    "                 all_compound_names = all_compound_names)\n",
    "            step6(org_dict = step4_output, genome_input = genome_input)\n",
    "            \n",
    "            # Set this to True, so that this input is included in step 7\n",
    "            genome_input[\"partB_network_complete\"] = True\n",
    "            \n",
    "    #-------------------------   \n",
    "    # Find summary statistics and comparison metrics and print to file\n",
    "    step7(all_inputs = all_inputs)\n",
    "    \n",
    "    #-------------------------   \n",
    "    # Print progress\n",
    "    print(\"Part B of Full Pipeline Complete\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acyrthosiphon pisum KEGG api\n",
      "Part B already complete for this genome.\n",
      "Buchnera aphidicola Tuc7 NCBI ASM2106v1\n",
      "Part B already complete for this genome.\n",
      "Buchnera aphidicola 5A NCBI ASM2108v1\n",
      "Part B already complete for this genome.\n",
      "Buchnera aphidicola APS NCBI ASM960v1\n",
      "Part B already complete for this genome.\n",
      "Regiella insecticola LSR1 NCBI ASM14362v1\n",
      "Part B already complete for this genome.\n",
      "Rickettsiella viridis Ap-RA04 NCBI ASM396675v1\n",
      "Part B already complete for this genome.\n",
      "Serratia symbiotica Tuscon NCBI ASM18648v1\n",
      "Part B already complete for this genome.\n",
      "Fukatsuia symbiotica 5D NCBI ASM312242v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa 5AT (AA) NCBI ASM2170v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa NY26 (AA) NCBI New_ASM277729v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa MI47 (BB) NCBI New_ASM226940v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa 5D (BB) NCBI New_ASM312244v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa MI12 (CC) NCBI New_ASM359054v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa A2C (DD) NCBI New_ASM277719v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa AS3 (DD) NCBI New_ASM277721v1\n",
      "Part B already complete for this genome.\n",
      "Hamiltonella defensa ZA17 (EE) NCBI New_ASM277723v1\n",
      "Part B already complete for this genome.\n",
      "Step 7 complete.\n",
      "Part B of Full Pipeline Complete\n",
      "CPU times: user 4.66 s, sys: 237 ms, total: 4.9 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%time full_pipeline_partB(all_inputs = all_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "# <span style=\"margin:auto;display:table;\">Network reconstruction is complete!</span>\n",
    "\n",
    "## <span style=\"margin:auto;display:table;\">Please open the file titled \"NetInteract (Community version).ipynb\" to continue.</span>\n",
    "\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
